{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "498c999c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# --- Add near the beginning ---\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Using device: cuda:0\")\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    print(\"CUDA not available. Using device: cpu\")\n",
    "    device = \"cpu\"\n",
    "# --- Use 'device' variable later ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e6cf090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded test split with 1 documents.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load the dataset\n",
    "# -------------------------------\n",
    "# Assume ds is a dictionary-like dataset with splits \"train\" and \"test\".\n",
    "# We use only the test split.\n",
    "# Each instance in ds['test'] is assumed to have 'context', 'question', and 'answer'.\n",
    "# Replace this with your actual data loading as needed.\n",
    "\n",
    "# Load the dataset with test and train splits.\n",
    "ds = load_dataset(\"neural-bridge/rag-dataset-12000\")\n",
    "n = 1  # Number of documents to select (adjust as needed)\n",
    "\n",
    "# Define a function to compute the context length.\n",
    "def add_context_length(example):\n",
    "    # Calculate the length of the context (number of characters)\n",
    "    example[\"context_length\"] = len(example[\"context\"])\n",
    "    return example\n",
    "\n",
    "# Add the new 'context_length' field to each example in the test split.\n",
    "test_data = ds['test'].map(add_context_length)\n",
    "\n",
    "# Sort the test split by the new context_length field in ascending order (shortest contexts first).\n",
    "test_data = test_data.sort(\"context_length\")\n",
    "\n",
    "# Select the top n documents with the shortest contexts.\n",
    "test_data = test_data.select(range(n))\n",
    "print(f\"Loaded test split with {len(test_data)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f16a6504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3643/3971842704.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  hf_embeddings = HuggingFaceEmbeddings(model_name=embed_model_name)\n",
      "2025-04-21 23:24:34.435391: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-21 23:24:34.502397: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-21 23:24:34.519003: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-21 23:24:34.523102: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-21 23:24:34.588307: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# ------------------------------------\n",
    "# 2. Prepare embeddings for retrieval\n",
    "# ------------------------------------\n",
    "embed_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name=embed_model_name)\n",
    "\n",
    "# Extract all passages in one flat list\n",
    "passages = []\n",
    "for ex in test_data:\n",
    "    passages.extend([ex[\"context\"]])\n",
    "    \n",
    "# print(passages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7453ed8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# -------------------------------------\n",
    "# 3. Build a FAISS vectorstore (in-memory)\n",
    "# -------------------------------------\n",
    "# vectorstore = FAISS.from_documents(documents=all_splits, embedding=hf_embeddings)\n",
    "vectorstore = FAISS.from_texts(texts=passages, embedding=hf_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2c3ab229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain.chains import RetrievalQA\n",
    "import torch\n",
    "\n",
    "# --------------------------------------\n",
    "# 3.1 Create text-gen pipeline\n",
    "# --------------------------------------\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer_id = model_id # Usually the same\n",
    "\n",
    "# Load tokenizer explicitly first to set pad token\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "\n",
    "# Make pipeline\n",
    "hf_gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    tokenizer=tokenizer_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device=0,\n",
    "    max_new_tokens=1024,\n",
    "    batch_size=20,\n",
    "    temperature=0.01,\n",
    "    do_sample=True,\n",
    ")\n",
    "hf_gen.tokenizer.pad_token = hf_gen.tokenizer.eos_token\n",
    "\n",
    "# Wrap it in LangChainâ€™s LLM interface\n",
    "llm = HuggingFacePipeline(pipeline=hf_gen, model_kwargs={\"stop\": [\"}\\n```\", \"}\\n\\n\", \"}\\nHere is\"]})\n",
    "\n",
    "# Wrap both in RAGAS wrappers (correct order now)\n",
    "hf_wrapped = LangchainLLMWrapper(llm)\n",
    "hf_embed_wrapped = LangchainEmbeddingsWrapper(hf_embeddings)\n",
    "\n",
    "# Build retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "# 4. Now create the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",                # or \"map_reduce\" / \"refine\" if you prefer\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0fdd6147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOLE RESPONSE:\n",
      "{'query': 'What is the latest version of Scorpion Solitaire?', 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nScorpion Solitaire\\n- User\\n- -\\n-\\n- Insufficient votes\\n- Softonic\\n- 6\\n- Not bad\\n- Not bad\\n- Your rating:\\n-\\nYour rating has been saved\\nOops, something's gone wrong. Try again.\\n- License:\\n- Free\\n- Language:\\n- OS:\\n-\\n- Latest version:\\n- 1.1 17/02/06\\n- Last month's downloads:\\n- 41\\n- -...\\n- PocketBalone\\n- Billiard Master\\n- Chess\\n- ...\\n- 21\\nScorpion Solitaire\\nSoftonic - Top Downloads\\nTop Downloads\\n- Pocket Uno\\nPlay the classic card game on your Pocket PC\\n- Multiplayer Championship Poker -...\\n- PocketBalone\\n- Billiard Master\\n- Trivial Pursuit\\n- ...\\n- 21\\nScorpion Solitaire.\\n\\nQuestion: What is the latest version of Scorpion Solitaire?\\nHelpful Answer: According to the Softonic website, the latest version of Scorpion Solitaire is 1.1, released on 17/02/06.\"}\n",
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Scorpion Solitaire\n",
      "- User\n",
      "- -\n",
      "-\n",
      "- Insufficient votes\n",
      "- Softonic\n",
      "- 6\n",
      "- Not bad\n",
      "- Not bad\n",
      "- Your rating:\n",
      "-\n",
      "Your rating has been saved\n",
      "Oops, something's gone wrong. Try again.\n",
      "- License:\n",
      "- Free\n",
      "- Language:\n",
      "- OS:\n",
      "-\n",
      "- Latest version:\n",
      "- 1.1 17/02/06\n",
      "- Last month's downloads:\n",
      "- 41\n",
      "- -...\n",
      "- PocketBalone\n",
      "- Billiard Master\n",
      "- Chess\n",
      "- ...\n",
      "- 21\n",
      "Scorpion Solitaire\n",
      "Softonic - Top Downloads\n",
      "Top Downloads\n",
      "- Pocket Uno\n",
      "Play the classic card game on your Pocket PC\n",
      "- Multiplayer Championship Poker -...\n",
      "- PocketBalone\n",
      "- Billiard Master\n",
      "- Trivial Pursuit\n",
      "- ...\n",
      "- 21\n",
      "Scorpion Solitaire.\n",
      "\n",
      "Question: What is the latest version of Scorpion Solitaire?\n",
      "Helpful Answer: According to the Softonic website, the latest version of Scorpion Solitaire is 1.1, released on 17/02/06.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# 3.2 Test it on the first user_input\n",
    "# --------------------------------------------------\n",
    "res = qa_chain({\"query\": test_data[\"question\"][0]})\n",
    "print(\"WHOLE RESPONSE:\")\n",
    "print(res)\n",
    "print(res[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d14fc7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RETREIVED CONTENT\n",
      "Scorpion Solitaire\n",
      "- User\n",
      "- -\n",
      "-\n",
      "- Insufficient votes\n",
      "- Softonic\n",
      "- 6\n",
      "- Not bad\n",
      "- Not bad\n",
      "- Your rating:\n",
      "-\n",
      "Your rating has been saved\n",
      "Oops, something's gone wrong. Try again.\n",
      "- License:\n",
      "- Free\n",
      "- Language:\n",
      "- OS:\n",
      "-\n",
      "- Latest version:\n",
      "- 1.1 17/02/06\n",
      "- Last month's downloads:\n",
      "- 41\n",
      "- -...\n",
      "- PocketBalone\n",
      "- Billiard Master\n",
      "- Chess\n",
      "- ...\n",
      "- 21\n",
      "Scorpion Solitaire\n",
      "Softonic - Top Downloads\n",
      "Top Downloads\n",
      "- Pocket Uno\n",
      "Play the classic card game on your Pocket PC\n",
      "- Multiplayer Championship Poker -...\n",
      "- PocketBalone\n",
      "- Billiard Master\n",
      "- Trivial Pursuit\n",
      "- ...\n",
      "- 21\n",
      "Scorpion Solitaire.\n",
      "SOURCE CONTENT\n",
      "Scorpion Solitaire\n",
      "- User\n",
      "- -\n",
      "-\n",
      "- Insufficient votes\n",
      "- Softonic\n",
      "- 6\n",
      "- Not bad\n",
      "- Not bad\n",
      "- Your rating:\n",
      "-\n",
      "Your rating has been saved\n",
      "Oops, something's gone wrong. Try again.\n",
      "- License:\n",
      "- Free\n",
      "- Language:\n",
      "- OS:\n",
      "-\n",
      "- Latest version:\n",
      "- 1.1 17/02/06\n",
      "- Last month's downloads:\n",
      "- 41\n",
      "- -...\n",
      "- PocketBalone\n",
      "- Billiard Master\n",
      "- Chess\n",
      "- ...\n",
      "- 21\n",
      "Scorpion Solitaire\n",
      "Softonic - Top Downloads\n",
      "Top Downloads\n",
      "- Pocket Uno\n",
      "Play the classic card game on your Pocket PC\n",
      "- Multiplayer Championship Poker -...\n",
      "- PocketBalone\n",
      "- Billiard Master\n",
      "- Trivial Pursuit\n",
      "- ...\n",
      "- 21\n",
      "Scorpion Solitaire.\n",
      "CONTEXT PRECISION\n",
      "0.9999999999\n",
      "CONTEXT RECALL\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import NonLLMContextPrecisionWithReference\n",
    "from ragas.metrics import NonLLMContextRecall\n",
    "from ragas import SingleTurnSample\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Experiment with evaluation\n",
    "# --------------------------------------------------\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Context Precision Test on the first question\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Instantiate an object, for some reason\n",
    "context_precision = NonLLMContextPrecisionWithReference()\n",
    "\n",
    "# THIS IS HOW YOU GET THE PAGE CONTENT FOR A RETRIEVED DOCUMENT, 0 HERE IS THE FIRST RESULT\n",
    "print(\"RETREIVED CONTENT\")\n",
    "print(res[\"source_documents\"][0].page_content) # this will need to be a list, in practice\n",
    "\n",
    "# THIS IS HOW YOU GET THE SOURCE CONTEXT, 0 HERE IS THE FIRST DOCUMENT\n",
    "print(\"SOURCE CONTENT\")\n",
    "print(test_data[\"context\"][0]) # this will need to be a list, in practice\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    retrieved_contexts=[res[\"source_documents\"][0].page_content], \n",
    "    reference_contexts=[test_data[\"context\"][0]]\n",
    ")\n",
    "print(\"CONTEXT PRECISION\")\n",
    "print(await context_precision.single_turn_ascore(sample))\n",
    "\n",
    "# HOLY SHIT IT WORKS!!!\n",
    "\n",
    "# -------------------------------\n",
    "# Context Recall Test\n",
    "# -------------------------------\n",
    "\n",
    "# stupid fucking object\n",
    "context_recall = NonLLMContextRecall()\n",
    "print(\"CONTEXT RECALL\")\n",
    "print(await context_recall.single_turn_ascore(sample))\n",
    "\n",
    "# I'm actually tony stark btw... I think we can do answer relevancy and faithfulness later??? \n",
    "# Right now, scale up these two...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7b2097",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SYSTEM]\n",
      "    TEXT: Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\n",
      "Please return the output in a JSON format that complies with the following schema as specified in JSON Schema and OpenAPI specification:\n",
      "{'$defs': {'SentenceComponents': {'properties': {'sentence_index': {'description': 'The index of the sentence', 'title': 'Sentence Index', 'type': 'integer'}, 'simpler_statements': {'description': 'A list of simpler statements that can be directly inferred from the context', 'items': {'type': 'string'}, 'title': 'Simpler Statements', 'type': 'array'}}, 'required': ['sentence_index', 'simpler_statements'], 'title': 'SentenceComponents', 'type': 'object'}}, 'properties': {'sentences': {'description': 'A list of sentences and their simpler versions', 'items': {'$ref': '#/$defs/SentenceComponents'}, 'title': 'Sentences', 'type': 'array'}}, 'required': ['sentences'], 'title': 'SentencesSimplified', 'type': 'object'}\n",
      "These are some examples to show how to perform the above instruction\n",
      "Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\n",
      "input: {\n",
      "    \"question\": \"Who was Albert Einstein and what is he best known for?\",\n",
      "    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\",\n",
      "    \"sentences\": {\n",
      "        \"0\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time.\",\n",
      "        \"1\": \"He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\n",
      "    }\n",
      "}\n",
      "output: {\n",
      "    \"sentences\": [\n",
      "        {\n",
      "            \"sentence_index\": 0,\n",
      "            \"simpler_statements\": [\n",
      "                \"Albert Einstein was a German-born theoretical physicist.\",\n",
      "                \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"sentence_index\": 1,\n",
      "            \"simpler_statements\": [\n",
      "                \"Albert Einstein was best known for developing the theory of relativity.\",\n",
      "                \"Albert Einstein also made important contributions to the development of the theory of quantum mechanics.\"\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "Now perform the above instruction with the following input\n",
      "input: {\n",
      "    \"question\": \"When was the first super bowl?\",\n",
      "    \"answer\": \"The first superbowl was held on Jan 15, 1967\",\n",
      "    \"sentences\": {}\n",
      "}\n",
      "Respond only with a valid JSON object that complies with the specified schema. **CRITICAL: Your entire response must be ONLY the single JSON object, starting with { and ending with }. Do not add any explanations, notes, or other text before or after the JSON object.**\n",
      "output:\n",
      "{\n",
      "    \"sentences\": [\n",
      "        {\n",
      "            \"sentence_index\": 0,\n",
      "            \"simpler_statements\": [\n",
      "                \"The first superbowl was held on January 15, 1967.\"\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "Extracted JSON:\n",
      "{\n",
      "  \"sentences\": [\n",
      "    {\n",
      "      \"sentence_index\": 0,\n",
      "      \"simpler_statements\": [\n",
      "        \"The first superbowl was held on January 15, 1967.\"\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# 4.1 Experiment with faithfulness\n",
    "# --------------------------------------------------\n",
    "\n",
    "# This prompt is pretty good for getting the number of claims\n",
    "prompt_text = \"\"\"\n",
    "[SYSTEM]\n",
    "    TEXT: Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\n",
    "Please return the output in a JSON format that complies with the following schema as specified in JSON Schema and OpenAPI specification:\n",
    "{'$defs': {'SentenceComponents': {'properties': {'sentence_index': {'description': 'The index of the sentence', 'title': 'Sentence Index', 'type': 'integer'}, 'simpler_statements': {'description': 'A list of simpler statements that can be directly inferred from the context', 'items': {'type': 'string'}, 'title': 'Simpler Statements', 'type': 'array'}}, 'required': ['sentence_index', 'simpler_statements'], 'title': 'SentenceComponents', 'type': 'object'}}, 'properties': {'sentences': {'description': 'A list of sentences and their simpler versions', 'items': {'$ref': '#/$defs/SentenceComponents'}, 'title': 'Sentences', 'type': 'array'}}, 'required': ['sentences'], 'title': 'SentencesSimplified', 'type': 'object'}\n",
    "These are some examples to show how to perform the above instruction\n",
    "Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\n",
    "input: {\n",
    "    \"question\": \"Who was Albert Einstein and what is he best known for?\",\n",
    "    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\",\n",
    "    \"sentences\": {\n",
    "        \"0\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time.\",\n",
    "        \"1\": \"He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\n",
    "    }\n",
    "}\n",
    "output: {\n",
    "    \"sentences\": [\n",
    "        {\n",
    "            \"sentence_index\": 0,\n",
    "            \"simpler_statements\": [\n",
    "                \"Albert Einstein was a German-born theoretical physicist.\",\n",
    "                \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"sentence_index\": 1,\n",
    "            \"simpler_statements\": [\n",
    "                \"Albert Einstein was best known for developing the theory of relativity.\",\n",
    "                \"Albert Einstein also made important contributions to the development of the theory of quantum mechanics.\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "Now perform the above instruction with the following input\n",
    "input: {\n",
    "    \"question\": \"When was the first super bowl?\",\n",
    "    \"answer\": \"The first superbowl was held on Jan 15, 1967\",\n",
    "    \"sentences\": {}\n",
    "}\n",
    "Respond only with a valid JSON object that complies with the specified schema. **CRITICAL: Your entire response must be ONLY the single JSON object, starting with { and ending with }. Do not add any explanations, notes, or other text before or after the JSON object.**\n",
    "output:\n",
    "\"\"\"\n",
    "raw = llm(prompt_text, return_full_text=False)\n",
    "print(raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3f88f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------------\n",
    "# 4.2 Extract the JSON object from the LLM's response\n",
    "# --------------------------------------------------\n",
    "import json\n",
    "import re\n",
    "\n",
    "def extract_json_from_output(text):\n",
    "    \"\"\"Extract the JSON object from the LLM's response.\"\"\"\n",
    "    # Find the last occurrence of a complete JSON object\n",
    "    json_pattern = r'\\{(?:[^{}]|(?:\\{(?:[^{}]|(?:\\{(?:[^{}]|(?:\\{[^{}]*\\}))*\\}))*\\}))*\\}'\n",
    "    matches = list(re.finditer(json_pattern, text))\n",
    "    \n",
    "    if matches:\n",
    "        # Get the last match (which should be our output JSON)\n",
    "        json_str = matches[-1].group(0)\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "json_result = extract_json_from_output(raw)\n",
    "\n",
    "# Print the extracted JSON in a readable format\n",
    "if json_result:\n",
    "    print(\"Extracted JSON:\")\n",
    "    print(json.dumps(json_result, indent=2))\n",
    "else:\n",
    "    print(\"OH SHIT\")\n",
    "    print(\"Failed to extract valid JSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "23d835d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SYSTEM]\n",
      "TEXT: You are a precise analyzer of factual claims. Your task is to evaluate if claims made in a given 'response' are supported by a provided 'context', identify claims unique to the context, and detail your reasoning meticulously.\n",
      "\n",
      "**CORE TASK:**\n",
      "1.  Strictly identify atomic claims made *only* within the `response` text.\n",
      "2.  Strictly identify atomic claims made *only* within the `context` text (i.e., not mentioned in the `response`).\n",
      "3.  Verify if the claims from the `response` are supported by the `context`.\n",
      "4.  Outline your step-by-step reasoning process clearly separating response analysis from context analysis.\n",
      "\n",
      "**Follow these steps sequentially and precisely:**\n",
      "\n",
      "1.  **Analyze Response ONLY:** Read *only* the `response` text. Break it down into the smallest possible distinct, atomic factual statements that are *directly stated*. List these identified statements internally. This forms the candidate pool for `response_claims`.\n",
      "2.  **Finalize `response_claims`:** Review the statements identified in Step 1. Ensure they are truly atomic and directly from the `response`. Populate the `response_claims` list in the JSON with these finalized claims. **Do not proceed until this step is complete and accurate.**\n",
      "3.  **Analyze Context ONLY:** Now, read the `context` text. Identify all distinct, atomic factual claims within it. List these internally.\n",
      "4.  **Identify `context_only_claims`:** Compare the full list of context claims (from Step 3) with the finalized `response_claims` list (from Step 2). Create the `context_only_claims` list by including only those context claims that are *NOT* present in `response_claims`.\n",
      "5.  **Verify `response_claims` against Context:** For *each* claim in the `response_claims` list (from Step 2), check if the `context` text fully supports it.\n",
      "    *   \"Supported\" means *all* information in the response claim is explicitly stated or directly inferable from the context.\n",
      "    *   \"Unsupported\" means the context is silent, contradicts, or only partially supports the response claim.\n",
      "    *   Populate `supported_response_claims` and `unsupported_response_claims` based *only* on this verification of `response_claims`.\n",
      "6.  **Generate Justifications for `response_claims`:** For *each* claim in `response_claims`, write a brief justification explaining *why* it is supported or unsupported, referencing the context comparison. Populate the `justifications` dictionary. **CRITICAL: Keys in this dictionary MUST exactly match the strings in the `response_claims` list.**\n",
      "7.  **Count `response_claims`:** Count the total number of claims in the `response_claims` list (`num_total_response_claims`). Count the number of claims in the `supported_response_claims` list (`num_supported_response_claims`). Prepare the justification strings for these counts.\n",
      "8.  **Outline Reasoning (Chain of Thought):** Describe the *exact* steps you took, emphasizing the separation of response analysis (Steps 1-2) from context analysis and comparison (Steps 3-7). Detail *what* claims were identified in the response *before* mentioning context analysis. Explain the verification process for *each* response claim. Populate the `chain_of_thought` field.\n",
      "9.  **Format Output:** Assemble the final JSON object ensuring all fields are present and in the specified order.\n",
      "\n",
      "**CRITICAL INSTRUCTIONS:**\n",
      "*   `response_claims` MUST contain ONLY atomic statements directly extracted from the `response`. Absolutely NO information from the `context` should influence this list.\n",
      "*   `context_only_claims` MUST contain ONLY atomic statements found in the `context` but demonstrably NOT in the `response`.\n",
      "*   Verification, justifications, counts, and count justifications apply ONLY to `response_claims`.\n",
      "*   Keys in the `justifications` dictionary MUST be exact string matches of claims found in the `response_claims` list.\n",
      "\n",
      "Input will contain a 'response' (text to analyze) and 'context' (reference information).\n",
      "\n",
      "Format your output as a single, valid JSON object with the following structure (Pay attention to the order):\n",
      "{\n",
      "  \"response\": \"the original response text\",\n",
      "  \"context\": \"the provided context\",\n",
      "  \"chain_of_thought\": \"Detailed step-by-step reasoning, explicitly separating response analysis from context analysis and verification.\",\n",
      "  \"response_claims\": [\n",
      "    \"atomic claim 1 extracted ONLY from response\",\n",
      "    // ... more claims solely from response\n",
      "  ],\n",
      "  \"context_only_claims\": [\n",
      "    \"claim A found ONLY in context, not in response\",\n",
      "    // ... more claims solely from context\n",
      "  ],\n",
      "  \"supported_response_claims\": [\n",
      "    \"response claim 1 verified against context\",\n",
      "    // ... more supported response claims\n",
      "  ],\n",
      "  \"unsupported_response_claims\": [\n",
      "    \"response claim 2 verified against context\",\n",
      "    // ... more unsupported response claims\n",
      "  ],\n",
      "  \"justifications\": {\n",
      "    // Keys MUST be claims from the 'response_claims' list\n",
      "    \"atomic claim 1 extracted ONLY from response\": \"Justification based on context\",\n",
      "    \"atomic claim 2 extracted ONLY from response\": \"Justification based on context (e.g., 'not in context', 'contradicted by context')\"\n",
      "    // ... justifications ONLY for response_claims\n",
      "  },\n",
      "  \"num_supported_response_claims_justification\": \"Explanation of how the supported count for response claims was derived.\",\n",
      "  \"num_supported_response_claims\": 1, // Integer count of supported response claims\n",
      "  \"num_total_response_claims_justification\": \"Explanation of how the total count for response claims was derived.\",\n",
      "  \"num_total_response_claims\": 2 // Integer count of total claims identified ONLY in the response\n",
      "}\n",
      "\n",
      "Be thorough. Your entire output must be ONLY the single JSON object, starting with { and ending with }. Do not add any explanations, notes, or other text before or after the JSON object.\n",
      "\n",
      "**EXAMPLES:** Pay close attention to the detailed `chain_of_thought`, the strict separation of claim sources in `response_claims` vs `context_only_claims`, the exact matching required for `justifications` keys, and the field order.\n",
      "\n",
      "input: {\n",
      "  \"response\": \"Marie Curie was a Polish-born physicist and chemist. She was the first woman to win a Nobel Prize.\",\n",
      "  \"context\": \"Marie SkÅ‚odowska Curie (7 November 1867 â€“ 4 July 1934) was a Polish and naturalized-French physicist and chemist who conducted pioneering research on radioactivity. She was the first woman to win a Nobel Prize, the first person to win Nobel Prizes in two scientific fields, and the only person to win Nobel Prizes in multiple scientific fields.\"\n",
      "}\n",
      "\n",
      "output: {\n",
      "  \"response\": \"Marie Curie was a Polish-born physicist and chemist. She was the first woman to win a Nobel Prize.\",\n",
      "  \"context\": \"Marie SkÅ‚odowska Curie (7 November 1867 â€“ 4 July 1934) was a Polish and naturalized-French physicist and chemist who conducted pioneering research on radioactivity. She was the first woman to win a Nobel Prize, the first person to win Nobel Prizes in two scientific fields, and the only person to win Nobel Prizes in multiple scientific fields.\",\n",
      "  \"chain_of_thought\": \"1. Analyzed Response ONLY: Identified two atomic claims directly stated: 'Marie Curie was a Polish-born physicist and chemist' and 'Marie Curie was the first woman to win a Nobel Prize'. 2. Finalized `response_claims` with these two claims. 3. Analyzed Context: Identified claims about full name, dates, French naturalization, radioactivity research, winning Nobel in two fields, being the only person to win in multiple fields. 4. Identified `context_only_claims`: Compared context claims (Step 3) to response claims (Step 2). Found claims about full name, dates, French naturalization, radioactivity, winning in two fields, and being the only person to win multiple are unique to the context. 5. Verified `response_claims`: Claim 1 ('Polish-born physicist and chemist') is supported by context. Claim 2 ('first woman to win Nobel Prize') is supported by context. 6. Generated Justifications: Created justifications for the two response claims based on context verification. Keys exactly match `response_claims`. 7. Counted `response_claims`: Total=2, Supported=2. Prepared count justifications. 8. Formatted JSON output.\",\n",
      "  \"response_claims\": [\n",
      "    \"Marie Curie was a Polish-born physicist and chemist\",\n",
      "    \"Marie Curie was the first woman to win a Nobel Prize\"\n",
      "  ],\n",
      "  \"context_only_claims\": [\n",
      "    \"Marie Curie's full name was Marie SkÅ‚odowska Curie\",\n",
      "    \"Marie Curie lived from 7 November 1867 to 4 July 1934\",\n",
      "    \"Marie Curie was also naturalized-French\",\n",
      "    \"Marie Curie conducted pioneering research on radioactivity\",\n",
      "    \"Marie Curie was the first person to win Nobel Prizes in two scientific fields\",\n",
      "    \"Marie Curie was the only person to win Nobel Prizes in multiple scientific fields\"\n",
      "  ],\n",
      "  \"supported_response_claims\": [\n",
      "    \"Marie Curie was a Polish-born physicist and chemist\",\n",
      "    \"Marie Curie was the first woman to win a Nobel Prize\"\n",
      "  ],\n",
      "  \"unsupported_response_claims\": [],\n",
      "  \"justifications\": {\n",
      "    \"Marie Curie was a Polish-born physicist and chemist\": \"in context - context confirms Polish physicist and chemist\",\n",
      "    \"Marie Curie was the first woman to win a Nobel Prize\": \"in context\"\n",
      "  },\n",
      "  \"num_supported_response_claims_justification\": \"Derived by comparing the 2 claims identified strictly from the response against the context. Both were found to be fully supported.\",\n",
      "  \"num_supported_response_claims\": 2,\n",
      "  \"num_total_response_claims_justification\": \"Derived by breaking down the response ONLY into distinct factual statements. 2 atomic claims were identified.\",\n",
      "  \"num_total_response_claims\": 2\n",
      "}\n",
      "\n",
      "input: {\n",
      "  \"response\": \"The Great Wall of China is 13,171 miles long and visible from space.\",\n",
      "  \"context\": \"The Great Wall of China is an ancient series of walls and fortifications spanning approximately 13,171 miles (21,196 kilometers). Construction began as early as the 7th century BCE. It attracts millions of tourists each year.\"\n",
      "}\n",
      "\n",
      "output: {\n",
      "  \"response\": \"The Great Wall of China is 13,171 miles long and visible from space.\",\n",
      "  \"context\": \"The Great Wall of China is an ancient series of walls and fortifications spanning approximately 13,171 miles (21,196 kilometers). Construction began as early as the 7th century BCE. It attracts millions of tourists each year.\",\n",
      "  \"chain_of_thought\": \"1. Analyzed Response ONLY: Identified two atomic claims directly stated: 'The Great Wall of China is 13,171 miles long' and 'The Great Wall of China is visible from space'. 2. Finalized `response_claims` with these two claims. 3. Analyzed Context: Identified claims about being ancient walls/fortifications, km length, construction start date, attracting millions of tourists. 4. Identified `context_only_claims`: Compared context claims (Step 3) to response claims (Step 2). Found claims about ancient walls, km length, construction date, and tourism are unique to the context. 5. Verified `response_claims`: Claim 1 ('13,171 miles long') is supported by context ('approximately 13,171 miles'). Claim 2 ('visible from space') is unsupported as context does not mention it. 6. Generated Justifications: Created justifications for the two response claims based on context verification. Keys exactly match `response_claims`. 7. Counted `response_claims`: Total=2, Supported=1. Prepared count justifications. 8. Formatted JSON output.\",\n",
      "  \"response_claims\": [\n",
      "    \"The Great Wall of China is 13,171 miles long\",\n",
      "    \"The Great Wall of China is visible from space\"\n",
      "  ],\n",
      "  \"context_only_claims\": [\n",
      "    \"The Great Wall of China is an ancient series of walls and fortifications\",\n",
      "    \"The Great Wall of China spans 21,196 kilometers\",\n",
      "    \"Construction began as early as the 7th century BCE\",\n",
      "    \"The Great Wall attracts millions of tourists each year\"\n",
      "  ],\n",
      "  \"supported_response_claims\": [\n",
      "    \"The Great Wall of China is 13,171 miles long\"\n",
      "  ],\n",
      "  \"unsupported_response_claims\": [\n",
      "    \"The Great Wall of China is visible from space\"\n",
      "  ],\n",
      "  \"justifications\": {\n",
      "    \"The Great Wall of China is 13,171 miles long\": \"in context - context states length is approximately 13,171 miles\",\n",
      "    \"The Great Wall of China is visible from space\": \"not in context - context does not mention visibility from space\"\n",
      "  },\n",
      "  \"num_supported_response_claims_justification\": \"Derived by comparing the 2 claims identified strictly from the response against the context. 1 claim was found to be fully supported.\",\n",
      "  \"num_supported_response_claims\": 1,\n",
      "  \"num_total_response_claims_justification\": \"Derived by breaking down the response ONLY into distinct factual statements. 2 atomic claims were identified.\",\n",
      "  \"num_total_response_claims\": 2\n",
      "}\n",
      "\n",
      "Now perform the above instruction with the following input:\n",
      "\n",
      "input: {\n",
      "    \"response\": \"William Shakespeare wrote famous plays like 'Hamlet' and 'Romeo and Juliet'. He was born in Stratford-upon-Avon in 1564 and died in 1616. He also invented over 1000 new words.\",\n",
      "     \"context\": \"William Shakespeare (bapt. 26 April 1564 â€“ 23 April 1616) was an English playwright, poet and actor. He is widely regarded as the greatest writer in the English language and the world's pre-eminent dramatist. He is often called England's national poet and the 'Bard of Avon'. His extant works consist of some 39 plays, 154 sonnets, three long narrative poems, and a few other verses. His plays include 'Hamlet', 'Othello', 'King Lear', and 'Macbeth', as well as 'Romeo and Juliet'. He was born and raised in Stratford-upon-Avon.\"\n",
      "}\n",
      "\n",
      "Respond only with a valid JSON object that complies with the specified schema.\n",
      "**CRITICAL: Your entire response must be ONLY the single JSON object, starting with { and ending with }.\n",
      "Do not add any explanations, notes, or other text before or after the JSON object.**\n",
      "\n",
      "output:\n",
      "{\n",
      "  \"response\": \"William Shakespeare wrote famous plays like 'Hamlet' and 'Romeo and Juliet'. He was born in Stratford-upon-Avon in 1564 and died in 1616. He also invented over 1000 new words.\",\n",
      "  \"context\": \"William Shakespeare (bapt. 26 April 1564 â€“ 23 April 1616) was an English playwright, poet and actor. He is widely regarded as the greatest writer in the English language and the world's pre-eminent dramatist. He is often called England's national poet and the 'Bard of Avon'. His extant works consist of some 39 plays, 154 sonnets, three long narrative poems, and a few other verses. His plays include 'Hamlet', 'Othello', 'King Lear', and 'Macbeth', as well as 'Romeo and Juliet'. He was born and raised in Stratford-upon-Avon.\",\n",
      "  \"chain_of_thought\": \"1. Analyzed Response ONLY: Identified two atomic claims directly stated: 'William Shakespeare wrote famous plays like 'Hamlet' and 'Romeo and Juliet'' and 'He was born in Stratford-upon-Avon in 1564 and died in 1616'. 2. Finalized `response_claims` with these two claims. 3. Analyzed Context: Identified claims about birth and death dates, Stratford-upon-Avon, being an English playwright, poet, actor, greatest writer, England's national poet, 'Bard of Avon', extant works, plays, sonnets, narrative poems, and invented words. 4. Identified `context_only_claims`: Compared context claims (Step 3) to response claims (Step 2). Found claims about birth and death dates, Stratford-upon-Avon, being an English playwright, poet, actor, greatest writer, England's national poet, 'Bard of Avon', extant works, plays, sonnets, narrative poems, and invented words are unique to the context. 5. Verified `response_claims`: Claim 1 ('William Shakespeare wrote famous plays like 'Hamlet' and 'Romeo and Juliet'') is supported by context. Claim 2 ('He was born in Stratford-upon-Avon in 1564 and died in 1616') is supported by context. 6. Generated Justifications: Created justifications for the two response claims based on context verification. Keys exactly match `response_claims`. 7. Counted `response_claims`: Total=2, Supported=2. Prepared count justifications. 8. Formatted JSON output.\",\n",
      "  \"response_claims\": [\n",
      "    \"William Shakespeare wrote famous plays like 'Hamlet' and 'Romeo and Juliet'\",\n",
      "    \"He was born in Stratford-upon-Avon in 1564 and died in 1616\"\n",
      "  ],\n",
      "  \"context_only_claims\": [\n",
      "    \"William Shakespeare (bapt. 26 April 1564 â€“ 23 April 1616) was an English playwright, poet and actor\",\n",
      "    \"He is widely regarded as the greatest writer in the English language and the world's pre-eminent dramatist\",\n",
      "    \"He is often called England's national poet and the 'Bard of Avon'\",\n",
      "    \"His extant works consist of some 39 plays, 154 sonnets, three long narrative poems, and a few other verses\",\n",
      "    \"His plays include 'Hamlet', 'Othello', 'King Lear', and 'Macbeth', as well as 'Romeo and Juliet'\",\n",
      "    \"He was born and raised in Stratford-upon-Avon\",\n",
      "    \"He invented over 1000 new words\"\n",
      "  ],\n",
      "  \"supported_response_claims\": [\n",
      "    \"William Shakespeare wrote famous plays like 'Hamlet' and 'Romeo and Juliet'\",\n",
      "    \"He was born in Stratford-upon-Avon in 1564 and died in 1616\"\n",
      "  ],\n",
      "  \"unsupported_response_claims\": [],\n",
      "  \"justifications\": {\n",
      "    \"William Shakespeare wrote famous plays like 'Hamlet' and 'Romeo and Juliet'\": \"in context - context states he wrote famous plays\",\n",
      "    \"He was born in Stratford-upon-Avon in 1564 and died in 1616\": \"in context - context states birth and death dates\"\n",
      "  },\n",
      "  \"num_supported_response_claims_justification\": \"Derived by comparing the 2 claims identified strictly from the response against the context. 2 claims were found to be fully supported.\",\n",
      "  \"num_supported_response_claims\": 2,\n",
      "  \"num_total_response_claims_justification\": \"Derived by breaking down the response ONLY into distinct factual statements. 2 atomic claims were identified.\",\n",
      "  \"num_total_response_claims\": 2\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# 4.3 Test faithfulness prompts\n",
    "# --------------------------------------------------\n",
    "\n",
    "FaithfulnessPrompt = \"\"\"\n",
    "[SYSTEM]\n",
    "TEXT: You are a precise analyzer of factual claims. Your task is to evaluate if claims made in a given 'response' are supported by a provided 'context', identify claims unique to the context, and detail your reasoning meticulously.\n",
    "\n",
    "**CORE TASK:**\n",
    "1.  Strictly identify atomic claims made *only* within the `response` text.\n",
    "2.  Strictly identify atomic claims made *only* within the `context` text (i.e., not mentioned in the `response`).\n",
    "3.  Verify if the claims from the `response` are supported by the `context`.\n",
    "4.  Outline your step-by-step reasoning process clearly separating response analysis from context analysis.\n",
    "\n",
    "**Follow these steps sequentially and precisely:**\n",
    "\n",
    "1.  **Analyze Response ONLY:** Read *only* the `response` text. Break it down into the smallest possible distinct, atomic factual statements that are *directly stated*. List these identified statements internally. This forms the candidate pool for `response_claims`.\n",
    "2.  **Finalize `response_claims`:** Review the statements identified in Step 1. Ensure they are truly atomic and directly from the `response`. Populate the `response_claims` list in the JSON with these finalized claims. **Do not proceed until this step is complete and accurate.**\n",
    "3.  **Analyze Context ONLY:** Now, read the `context` text. Identify all distinct, atomic factual claims within it. List these internally.\n",
    "4.  **Identify `context_only_claims`:** Compare the full list of context claims (from Step 3) with the finalized `response_claims` list (from Step 2). Create the `context_only_claims` list by including only those context claims that are *NOT* present in `response_claims`.\n",
    "5.  **Verify `response_claims` against Context:** For *each* claim in the `response_claims` list (from Step 2), check if the `context` text fully supports it.\n",
    "    *   \"Supported\" means *all* information in the response claim is explicitly stated or directly inferable from the context.\n",
    "    *   \"Unsupported\" means the context is silent, contradicts, or only partially supports the response claim.\n",
    "    *   Populate `supported_response_claims` and `unsupported_response_claims` based *only* on this verification of `response_claims`.\n",
    "6.  **Generate Justifications for `response_claims`:** For *each* claim in `response_claims`, write a brief justification explaining *why* it is supported or unsupported, referencing the context comparison. Populate the `justifications` dictionary. **CRITICAL: Keys in this dictionary MUST exactly match the strings in the `response_claims` list.**\n",
    "7.  **Count `response_claims`:** Count the total number of claims in the `response_claims` list (`num_total_response_claims`). Count the number of claims in the `supported_response_claims` list (`num_supported_response_claims`). Prepare the justification strings for these counts.\n",
    "8.  **Outline Reasoning (Chain of Thought):** Describe the *exact* steps you took, emphasizing the separation of response analysis (Steps 1-2) from context analysis and comparison (Steps 3-7). Detail *what* claims were identified in the response *before* mentioning context analysis. Explain the verification process for *each* response claim. Populate the `chain_of_thought` field.\n",
    "9.  **Format Output:** Assemble the final JSON object ensuring all fields are present and in the specified order.\n",
    "\n",
    "**CRITICAL INSTRUCTIONS:**\n",
    "*   `response_claims` MUST contain ONLY atomic statements directly extracted from the `response`. Absolutely NO information from the `context` should influence this list.\n",
    "*   `context_only_claims` MUST contain ONLY atomic statements found in the `context` but demonstrably NOT in the `response`.\n",
    "*   Verification, justifications, counts, and count justifications apply ONLY to `response_claims`.\n",
    "*   Keys in the `justifications` dictionary MUST be exact string matches of claims found in the `response_claims` list.\n",
    "\n",
    "Input will contain a 'response' (text to analyze) and 'context' (reference information).\n",
    "\n",
    "Format your output as a single, valid JSON object with the following structure (Pay attention to the order):\n",
    "{\n",
    "  \"response\": \"the original response text\",\n",
    "  \"context\": \"the provided context\",\n",
    "  \"chain_of_thought\": \"Detailed step-by-step reasoning, explicitly separating response analysis from context analysis and verification.\",\n",
    "  \"response_claims\": [\n",
    "    \"atomic claim 1 extracted ONLY from response\",\n",
    "    // ... more claims solely from response\n",
    "  ],\n",
    "  \"context_only_claims\": [\n",
    "    \"claim A found ONLY in context, not in response\",\n",
    "    // ... more claims solely from context\n",
    "  ],\n",
    "  \"supported_response_claims\": [\n",
    "    \"response claim 1 verified against context\",\n",
    "    // ... more supported response claims\n",
    "  ],\n",
    "  \"unsupported_response_claims\": [\n",
    "    \"response claim 2 verified against context\",\n",
    "    // ... more unsupported response claims\n",
    "  ],\n",
    "  \"justifications\": {\n",
    "    // Keys MUST be claims from the 'response_claims' list\n",
    "    \"atomic claim 1 extracted ONLY from response\": \"Justification based on context\",\n",
    "    \"atomic claim 2 extracted ONLY from response\": \"Justification based on context (e.g., 'not in context', 'contradicted by context')\"\n",
    "    // ... justifications ONLY for response_claims\n",
    "  },\n",
    "  \"num_supported_response_claims_justification\": \"Explanation of how the supported count for response claims was derived.\",\n",
    "  \"num_supported_response_claims\": 1, // Integer count of supported response claims\n",
    "  \"num_total_response_claims_justification\": \"Explanation of how the total count for response claims was derived.\",\n",
    "  \"num_total_response_claims\": 2 // Integer count of total claims identified ONLY in the response\n",
    "}\n",
    "\n",
    "Be thorough. Your entire output must be ONLY the single JSON object, starting with { and ending with }. Do not add any explanations, notes, or other text before or after the JSON object.\n",
    "\n",
    "**EXAMPLES:** Pay close attention to the detailed `chain_of_thought`, the strict separation of claim sources in `response_claims` vs `context_only_claims`, the exact matching required for `justifications` keys, and the field order.\n",
    "\n",
    "input: {\n",
    "  \"response\": \"Marie Curie was a Polish-born physicist and chemist. She was the first woman to win a Nobel Prize.\",\n",
    "  \"context\": \"Marie SkÅ‚odowska Curie (7 November 1867 â€“ 4 July 1934) was a Polish and naturalized-French physicist and chemist who conducted pioneering research on radioactivity. She was the first woman to win a Nobel Prize, the first person to win Nobel Prizes in two scientific fields, and the only person to win Nobel Prizes in multiple scientific fields.\"\n",
    "}\n",
    "\n",
    "output: {\n",
    "  \"response\": \"Marie Curie was a Polish-born physicist and chemist. She was the first woman to win a Nobel Prize.\",\n",
    "  \"context\": \"Marie SkÅ‚odowska Curie (7 November 1867 â€“ 4 July 1934) was a Polish and naturalized-French physicist and chemist who conducted pioneering research on radioactivity. She was the first woman to win a Nobel Prize, the first person to win Nobel Prizes in two scientific fields, and the only person to win Nobel Prizes in multiple scientific fields.\",\n",
    "  \"chain_of_thought\": \"1. Analyzed Response ONLY: Identified two atomic claims directly stated: 'Marie Curie was a Polish-born physicist and chemist' and 'Marie Curie was the first woman to win a Nobel Prize'. 2. Finalized `response_claims` with these two claims. 3. Analyzed Context: Identified claims about full name, dates, French naturalization, radioactivity research, winning Nobel in two fields, being the only person to win in multiple fields. 4. Identified `context_only_claims`: Compared context claims (Step 3) to response claims (Step 2). Found claims about full name, dates, French naturalization, radioactivity, winning in two fields, and being the only person to win multiple are unique to the context. 5. Verified `response_claims`: Claim 1 ('Polish-born physicist and chemist') is supported by context. Claim 2 ('first woman to win Nobel Prize') is supported by context. 6. Generated Justifications: Created justifications for the two response claims based on context verification. Keys exactly match `response_claims`. 7. Counted `response_claims`: Total=2, Supported=2. Prepared count justifications. 8. Formatted JSON output.\",\n",
    "  \"response_claims\": [\n",
    "    \"Marie Curie was a Polish-born physicist and chemist\",\n",
    "    \"Marie Curie was the first woman to win a Nobel Prize\"\n",
    "  ],\n",
    "  \"context_only_claims\": [\n",
    "    \"Marie Curie's full name was Marie SkÅ‚odowska Curie\",\n",
    "    \"Marie Curie lived from 7 November 1867 to 4 July 1934\",\n",
    "    \"Marie Curie was also naturalized-French\",\n",
    "    \"Marie Curie conducted pioneering research on radioactivity\",\n",
    "    \"Marie Curie was the first person to win Nobel Prizes in two scientific fields\",\n",
    "    \"Marie Curie was the only person to win Nobel Prizes in multiple scientific fields\"\n",
    "  ],\n",
    "  \"supported_response_claims\": [\n",
    "    \"Marie Curie was a Polish-born physicist and chemist\",\n",
    "    \"Marie Curie was the first woman to win a Nobel Prize\"\n",
    "  ],\n",
    "  \"unsupported_response_claims\": [],\n",
    "  \"justifications\": {\n",
    "    \"Marie Curie was a Polish-born physicist and chemist\": \"in context - context confirms Polish physicist and chemist\",\n",
    "    \"Marie Curie was the first woman to win a Nobel Prize\": \"in context\"\n",
    "  },\n",
    "  \"num_supported_response_claims_justification\": \"Derived by comparing the 2 claims identified strictly from the response against the context. Both were found to be fully supported.\",\n",
    "  \"num_supported_response_claims\": 2,\n",
    "  \"num_total_response_claims_justification\": \"Derived by breaking down the response ONLY into distinct factual statements. 2 atomic claims were identified.\",\n",
    "  \"num_total_response_claims\": 2\n",
    "}\n",
    "\n",
    "input: {\n",
    "  \"response\": \"The Great Wall of China is 13,171 miles long and visible from space.\",\n",
    "  \"context\": \"The Great Wall of China is an ancient series of walls and fortifications spanning approximately 13,171 miles (21,196 kilometers). Construction began as early as the 7th century BCE. It attracts millions of tourists each year.\"\n",
    "}\n",
    "\n",
    "output: {\n",
    "  \"response\": \"The Great Wall of China is 13,171 miles long and visible from space.\",\n",
    "  \"context\": \"The Great Wall of China is an ancient series of walls and fortifications spanning approximately 13,171 miles (21,196 kilometers). Construction began as early as the 7th century BCE. It attracts millions of tourists each year.\",\n",
    "  \"chain_of_thought\": \"1. Analyzed Response ONLY: Identified two atomic claims directly stated: 'The Great Wall of China is 13,171 miles long' and 'The Great Wall of China is visible from space'. 2. Finalized `response_claims` with these two claims. 3. Analyzed Context: Identified claims about being ancient walls/fortifications, km length, construction start date, attracting millions of tourists. 4. Identified `context_only_claims`: Compared context claims (Step 3) to response claims (Step 2). Found claims about ancient walls, km length, construction date, and tourism are unique to the context. 5. Verified `response_claims`: Claim 1 ('13,171 miles long') is supported by context ('approximately 13,171 miles'). Claim 2 ('visible from space') is unsupported as context does not mention it. 6. Generated Justifications: Created justifications for the two response claims based on context verification. Keys exactly match `response_claims`. 7. Counted `response_claims`: Total=2, Supported=1. Prepared count justifications. 8. Formatted JSON output.\",\n",
    "  \"response_claims\": [\n",
    "    \"The Great Wall of China is 13,171 miles long\",\n",
    "    \"The Great Wall of China is visible from space\"\n",
    "  ],\n",
    "  \"context_only_claims\": [\n",
    "    \"The Great Wall of China is an ancient series of walls and fortifications\",\n",
    "    \"The Great Wall of China spans 21,196 kilometers\",\n",
    "    \"Construction began as early as the 7th century BCE\",\n",
    "    \"The Great Wall attracts millions of tourists each year\"\n",
    "  ],\n",
    "  \"supported_response_claims\": [\n",
    "    \"The Great Wall of China is 13,171 miles long\"\n",
    "  ],\n",
    "  \"unsupported_response_claims\": [\n",
    "    \"The Great Wall of China is visible from space\"\n",
    "  ],\n",
    "  \"justifications\": {\n",
    "    \"The Great Wall of China is 13,171 miles long\": \"in context - context states length is approximately 13,171 miles\",\n",
    "    \"The Great Wall of China is visible from space\": \"not in context - context does not mention visibility from space\"\n",
    "  },\n",
    "  \"num_supported_response_claims_justification\": \"Derived by comparing the 2 claims identified strictly from the response against the context. 1 claim was found to be fully supported.\",\n",
    "  \"num_supported_response_claims\": 1,\n",
    "  \"num_total_response_claims_justification\": \"Derived by breaking down the response ONLY into distinct factual statements. 2 atomic claims were identified.\",\n",
    "  \"num_total_response_claims\": 2\n",
    "}\n",
    "\n",
    "Now perform the above instruction with the following input:\n",
    "\n",
    "input: {\n",
    "    \"response\": \"William Shakespeare wrote famous plays like 'Hamlet' and 'Romeo and Juliet'. He was born in Stratford-upon-Avon in 1564 and died in 1616. He also invented over 1000 new words.\",\n",
    "     \"context\": \"William Shakespeare (bapt. 26 April 1564 â€“ 23 April 1616) was an English playwright, poet and actor. He is widely regarded as the greatest writer in the English language and the world's pre-eminent dramatist. He is often called England's national poet and the 'Bard of Avon'. His extant works consist of some 39 plays, 154 sonnets, three long narrative poems, and a few other verses. His plays include 'Hamlet', 'Othello', 'King Lear', and 'Macbeth', as well as 'Romeo and Juliet'. He was born and raised in Stratford-upon-Avon.\"\n",
    "}\n",
    "\n",
    "Respond only with a valid JSON object that complies with the specified schema.\n",
    "**CRITICAL: Your entire response must be ONLY the single JSON object, starting with { and ending with }.\n",
    "Do not add any explanations, notes, or other text before or after the JSON object.**\n",
    "\n",
    "output:\n",
    "\"\"\"\n",
    "\n",
    "raw_faith = llm(FaithfulnessPrompt, return_full_text=False)\n",
    "print(raw_faith)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dc9b66ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted JSON:\n",
      "{\n",
      "  \"response\": \"The first superbowl was held on Jan 15, 1967\",\n",
      "  \"context\": \"The first Super Bowl, known as Super Bowl I, was played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles, California. The game featured the Green Bay Packers and the Kansas City Chiefs.\",\n",
      "  \"chain_of_thought\": \"1. Analyzed Response ONLY: Identified one atomic claim directly stated: 'The first superbowl was held on Jan 15, 1967'. 2. Finalized `response_claims` with this claim. 3. Analyzed Context: Identified claims about date, location, featured teams, game. 4. Identified `context_only_claims`: Compared context claims (Step 3) to response claims (Step 2). Found claims about date and location are unique to the context. 5. Verified `response_claims`: Claim 1 ('The first superbowl was held on Jan 15, 1967') is supported by context ('played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles, California'). 6. Generated Justifications: Created justifications for the one response claim based on context verification. Keys exactly match `response_claims`. 7. Counted `response_claims`: Total=1, Supported=1. Prepared count justifications. 8. Formatted JSON output\",\n",
      "  \"response_claims\": [\n",
      "    \"The first superbowl was held on Jan 15, 1967\"\n",
      "  ],\n",
      "  \"context_only_claims\": [\n",
      "    \"The first Super Bowl, known as Super Bowl I, was played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles, California\"\n",
      "  ],\n",
      "  \"supported_response_claims\": [\n",
      "    \"The first superbowl was held on Jan 15, 1967\"\n",
      "  ],\n",
      "  \"unsupported_response_claims\": [],\n",
      "  \"justifications\": {\n",
      "    \"The first superbowl was held on Jan 15, 1967\": \"in context - context confirms the date and location of the first Super Bowl\"\n",
      "  },\n",
      "  \"num_supported_response_claims_justification\": \"Derived by comparing the 1 claim identified strictly from the response against the context. 1 claim was found to be fully supported.\",\n",
      "  \"num_supported_response_claims\": 1,\n",
      "  \"num_total_response_claims_justification\": \"Derived by breaking down the response ONLY into distinct factual statements. 1 atomic claim was identified.\",\n",
      "  \"num_total_response_claims\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# 4.3 More Faithfulness work\n",
    "# --------------------------------------------------\n",
    "\n",
    "json_faith = extract_json_from_output(raw)\n",
    "\n",
    "# Print the extracted JSON in a readable format\n",
    "if json_faith:\n",
    "    print(\"Extracted JSON:\")\n",
    "    print(json.dumps(json_faith, indent=2))\n",
    "else:\n",
    "    print(\"OH SHIT\")\n",
    "    print(\"Failed to extract valid JSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d93485d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# 4.4 Develop Faithfulness Prompt\n",
    "# --------------------------------------------------\n",
    "def create_faithfulness_prompt(response: str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates the faithfulness evaluation prompt with the provided\n",
    "    response and context strings embedded.\n",
    "\n",
    "    Args:\n",
    "        response: The response string generated by the LLM.\n",
    "        context: The context string used for RAG.\n",
    "\n",
    "    Returns:\n",
    "        The complete prompt string ready for the LLM.\n",
    "    \"\"\"\n",
    "    # Escape the response and context for safe JSON embedding\n",
    "    response_escaped = json.dumps(response)\n",
    "    context_escaped = json.dumps(context)\n",
    "\n",
    "    # Define the prompt template using an f-string\n",
    "    # Note: Literal curly braces { } needed for the JSON examples within the\n",
    "    # prompt must be escaped by doubling them {{ }}\n",
    "    prompt = f\"\"\"\n",
    "[SYSTEM]\n",
    "TEXT: You are a precise analyzer of factual claims. Your task is to evaluate if claims made in a given 'response' are supported by a provided 'context', identify claims unique to the context, and detail your reasoning meticulously.\n",
    "\n",
    "**CORE TASK:**\n",
    "1.  Strictly identify atomic claims made *only* within the `response` text.\n",
    "2.  Strictly identify atomic claims made *only* within the `context` text (i.e., not mentioned in the `response`).\n",
    "3.  Verify if the claims from the `response` are supported by the `context`.\n",
    "4.  Outline your step-by-step reasoning process clearly separating response analysis from context analysis.\n",
    "\n",
    "**Follow these steps sequentially and precisely:**\n",
    "\n",
    "1.  **Analyze Response ONLY:** Read *only* the `response` text. Break it down into the smallest possible distinct, atomic factual statements that are *directly stated*. List these identified statements internally. This forms the candidate pool for `response_claims`.\n",
    "2.  **Finalize `response_claims`:** Review the statements identified in Step 1. Ensure they are truly atomic and directly from the `response`. Populate the `response_claims` list in the JSON with these finalized claims. **Do not proceed until this step is complete and accurate.**\n",
    "3.  **Analyze Context ONLY:** Now, read the `context` text. Identify all distinct, atomic factual claims within it. List these internally.\n",
    "4.  **Identify `context_only_claims`:** Compare the full list of context claims (from Step 3) with the finalized `response_claims` list (from Step 2). Create the `context_only_claims` list by including only those context claims that are *NOT* present in `response_claims`.\n",
    "5.  **Verify `response_claims` against Context:** For *each* claim in the `response_claims` list (from Step 2), check if the `context` text fully supports it.\n",
    "    *   \"Supported\" means *all* information in the response claim is explicitly stated or directly inferable from the context.\n",
    "    *   \"Unsupported\" means the context is silent, contradicts, or only partially supports the response claim.\n",
    "    *   Populate `supported_response_claims` and `unsupported_response_claims` based *only* on this verification of `response_claims`.\n",
    "6.  **Generate Justifications for `response_claims`:** For *each* claim in `response_claims`, write a brief justification explaining *why* it is supported or unsupported, referencing the context comparison. Populate the `justifications` dictionary. **CRITICAL: Keys in this dictionary MUST exactly match the strings in the `response_claims` list.**\n",
    "7.  **Count `response_claims`:** Count the total number of claims in the `response_claims` list (`num_total_response_claims`). Count the number of claims in the `supported_response_claims` list (`num_supported_response_claims`). Prepare the justification strings for these counts.\n",
    "8.  **Outline Reasoning (Chain of Thought):** Describe the *exact* steps you took, emphasizing the separation of response analysis (Steps 1-2) from context analysis and comparison (Steps 3-7). Detail *what* claims were identified in the response *before* mentioning context analysis. Explain the verification process for *each* response claim. Populate the `chain_of_thought` field.\n",
    "9.  **Format Output:** Assemble the final JSON object ensuring all fields are present and in the specified order.\n",
    "\n",
    "**CRITICAL INSTRUCTIONS:**\n",
    "*   `response_claims` MUST contain ONLY atomic statements directly extracted from the `response`. Absolutely NO information from the `context` should influence this list.\n",
    "*   `context_only_claims` MUST contain ONLY atomic statements found in the `context` but demonstrably NOT in the `response`.\n",
    "*   Verification, justifications, counts, and count justifications apply ONLY to `response_claims`.\n",
    "*   Keys in the `justifications` dictionary MUST be exact string matches of claims found in the `response_claims` list.\n",
    "\n",
    "Input will contain a 'response' (text to analyze) and 'context' (reference information).\n",
    "\n",
    "Format your output as a single, valid JSON object with the following structure (Pay attention to the order):\n",
    "{{\n",
    "  \"response\": \"the original response text\",\n",
    "  \"context\": \"the provided context\",\n",
    "  \"chain_of_thought\": \"Detailed step-by-step reasoning, explicitly separating response analysis from context analysis and verification.\",\n",
    "  \"response_claims\": [\n",
    "    \"atomic claim 1 extracted ONLY from response\",\n",
    "    // ... more claims solely from response\n",
    "  ],\n",
    "  \"context_only_claims\": [\n",
    "    \"claim A found ONLY in context, not in response\",\n",
    "    // ... more claims solely from context\n",
    "  ],\n",
    "  \"supported_response_claims\": [\n",
    "    \"response claim 1 verified against context\",\n",
    "    // ... more supported response claims\n",
    "  ],\n",
    "  \"unsupported_response_claims\": [\n",
    "    \"response claim 2 verified against context\",\n",
    "    // ... more unsupported response claims\n",
    "  ],\n",
    "  \"justifications\": {{\n",
    "    // Keys MUST be claims from the 'response_claims' list\n",
    "    \"atomic claim 1 extracted ONLY from response\": \"Justification based on context\",\n",
    "    \"atomic claim 2 extracted ONLY from response\": \"Justification based on context (e.g., 'not in context', 'contradicted by context')\"\n",
    "    // ... justifications ONLY for response_claims\n",
    "  }},\n",
    "  \"num_supported_response_claims_justification\": \"Explanation of how the supported count for response claims was derived.\",\n",
    "  \"num_supported_response_claims\": 1, // Integer count of supported response claims\n",
    "  \"num_total_response_claims_justification\": \"Explanation of how the total count for response claims was derived.\",\n",
    "  \"num_total_response_claims\": 2 // Integer count of total claims identified ONLY in the response\n",
    "}}\n",
    "\n",
    "Be thorough. Your entire output must be ONLY the single JSON object, starting with {{ and ending with }}. Do not add any explanations, notes, or other text before or after the JSON object.\n",
    "\n",
    "**EXAMPLES:** Pay close attention to the detailed `chain_of_thought`, the strict separation of claim sources in `response_claims` vs `context_only_claims`, the exact matching required for `justifications` keys, and the field order.\n",
    "\n",
    "input: {{\n",
    "  \"response\": \"Marie Curie was a Polish-born physicist and chemist. She was the first woman to win a Nobel Prize.\",\n",
    "  \"context\": \"Marie SkÅ‚odowska Curie (7 November 1867 â€“ 4 July 1934) was a Polish and naturalized-French physicist and chemist who conducted pioneering research on radioactivity. She was the first woman to win a Nobel Prize, the first person to win Nobel Prizes in two scientific fields, and the only person to win Nobel Prizes in multiple scientific fields.\"\n",
    "}}\n",
    "\n",
    "output: {{\n",
    "  \"response\": \"Marie Curie was a Polish-born physicist and chemist. She was the first woman to win a Nobel Prize.\",\n",
    "  \"context\": \"Marie SkÅ‚odowska Curie (7 November 1867 â€“ 4 July 1934) was a Polish and naturalized-French physicist and chemist who conducted pioneering research on radioactivity. She was the first woman to win a Nobel Prize, the first person to win Nobel Prizes in two scientific fields, and the only person to win Nobel Prizes in multiple scientific fields.\",\n",
    "  \"chain_of_thought\": \"1. Analyzed Response ONLY: Identified two atomic claims directly stated: 'Marie Curie was a Polish-born physicist and chemist' and 'Marie Curie was the first woman to win a Nobel Prize'. 2. Finalized `response_claims` with these two claims. 3. Analyzed Context: Identified claims about full name, dates, French naturalization, radioactivity research, winning Nobel in two fields, being the only person to win in multiple fields. 4. Identified `context_only_claims`: Compared context claims (Step 3) to response claims (Step 2). Found claims about full name, dates, French naturalization, radioactivity, winning in two fields, and being the only person to win multiple are unique to the context. 5. Verified `response_claims`: Claim 1 ('Polish-born physicist and chemist') is supported by context. Claim 2 ('first woman to win Nobel Prize') is supported by context. 6. Generated Justifications: Created justifications for the two response claims based on context verification. Keys exactly match `response_claims`. 7. Counted `response_claims`: Total=2, Supported=2. Prepared count justifications. 8. Formatted JSON output.\",\n",
    "  \"response_claims\": [\n",
    "    \"Marie Curie was a Polish-born physicist and chemist\",\n",
    "    \"Marie Curie was the first woman to win a Nobel Prize\"\n",
    "  ],\n",
    "  \"context_only_claims\": [\n",
    "    \"Marie Curie's full name was Marie SkÅ‚odowska Curie\",\n",
    "    \"Marie Curie lived from 7 November 1867 to 4 July 1934\",\n",
    "    \"Marie Curie was also naturalized-French\",\n",
    "    \"Marie Curie conducted pioneering research on radioactivity\",\n",
    "    \"Marie Curie was the first person to win Nobel Prizes in two scientific fields\",\n",
    "    \"Marie Curie was the only person to win Nobel Prizes in multiple scientific fields\"\n",
    "  ],\n",
    "  \"supported_response_claims\": [\n",
    "    \"Marie Curie was a Polish-born physicist and chemist\",\n",
    "    \"Marie Curie was the first woman to win a Nobel Prize\"\n",
    "  ],\n",
    "  \"unsupported_response_claims\": [],\n",
    "  \"justifications\": {{\n",
    "    \"Marie Curie was a Polish-born physicist and chemist\": \"in context - context confirms Polish physicist and chemist\",\n",
    "    \"Marie Curie was the first woman to win a Nobel Prize\": \"in context\"\n",
    "  }},\n",
    "  \"num_supported_response_claims_justification\": \"Derived by comparing the 2 claims identified strictly from the response against the context. Both were found to be fully supported.\",\n",
    "  \"num_supported_response_claims\": 2,\n",
    "  \"num_total_response_claims_justification\": \"Derived by breaking down the response ONLY into distinct factual statements. 2 atomic claims were identified.\",\n",
    "  \"num_total_response_claims\": 2\n",
    "}}\n",
    "\n",
    "input: {{\n",
    "  \"response\": \"The Great Wall of China is 13,171 miles long and visible from space.\",\n",
    "  \"context\": \"The Great Wall of China is an ancient series of walls and fortifications spanning approximately 13,171 miles (21,196 kilometers). Construction began as early as the 7th century BCE. It attracts millions of tourists each year.\"\n",
    "}}\n",
    "\n",
    "output: {{\n",
    "  \"response\": \"The Great Wall of China is 13,171 miles long and visible from space.\",\n",
    "  \"context\": \"The Great Wall of China is an ancient series of walls and fortifications spanning approximately 13,171 miles (21,196 kilometers). Construction began as early as the 7th century BCE. It attracts millions of tourists each year.\",\n",
    "  \"chain_of_thought\": \"1. Analyzed Response ONLY: Identified two atomic claims directly stated: 'The Great Wall of China is 13,171 miles long' and 'The Great Wall of China is visible from space'. 2. Finalized `response_claims` with these two claims. 3. Analyzed Context: Identified claims about being ancient walls/fortifications, km length, construction start date, attracting millions of tourists. 4. Identified `context_only_claims`: Compared context claims (Step 3) to response claims (Step 2). Found claims about ancient walls, km length, construction date, and tourism are unique to the context. 5. Verified `response_claims`: Claim 1 ('13,171 miles long') is supported by context ('approximately 13,171 miles'). Claim 2 ('visible from space') is unsupported as context does not mention it. 6. Generated Justifications: Created justifications for the two response claims based on context verification. Keys exactly match `response_claims`. 7. Counted `response_claims`: Total=2, Supported=1. Prepared count justifications. 8. Formatted JSON output.\",\n",
    "  \"response_claims\": [\n",
    "    \"The Great Wall of China is 13,171 miles long\",\n",
    "    \"The Great Wall of China is visible from space\"\n",
    "  ],\n",
    "  \"context_only_claims\": [\n",
    "    \"The Great Wall of China is an ancient series of walls and fortifications\",\n",
    "    \"The Great Wall of China spans 21,196 kilometers\",\n",
    "    \"Construction began as early as the 7th century BCE\",\n",
    "    \"The Great Wall attracts millions of tourists each year\"\n",
    "  ],\n",
    "  \"supported_response_claims\": [\n",
    "    \"The Great Wall of China is 13,171 miles long\"\n",
    "  ],\n",
    "  \"unsupported_response_claims\": [\n",
    "    \"The Great Wall of China is visible from space\"\n",
    "  ],\n",
    "  \"justifications\": {{\n",
    "    \"The Great Wall of China is 13,171 miles long\": \"in context - context states length is approximately 13,171 miles\",\n",
    "    \"The Great Wall of China is visible from space\": \"not in context - context does not mention visibility from space\"\n",
    "  }},\n",
    "  \"num_supported_response_claims_justification\": \"Derived by comparing the 2 claims identified strictly from the response against the context. 1 claim was found to be fully supported.\",\n",
    "  \"num_supported_response_claims\": 1,\n",
    "  \"num_total_response_claims_justification\": \"Derived by breaking down the response ONLY into distinct factual statements. 2 atomic claims were identified.\",\n",
    "  \"num_total_response_claims\": 2\n",
    "}}\n",
    "\n",
    "Now perform the above instruction with the following input:\n",
    "\n",
    "input: {{\n",
    "    \"response\": {response_escaped},\n",
    "    \"context\": {context_escaped}\n",
    "}}\n",
    "\n",
    "Respond only with a valid JSON object that complies with the specified schema.\n",
    "**CRITICAL: Your entire response must be ONLY the single JSON object, starting with {{ and ending with }}.\n",
    "Do not add any explanations, notes, or other text before or after the JSON object.**\n",
    "\n",
    "output:\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f085437f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXTRACTED ANSWER:\n",
      "According to the Softonic website, the latest version of Scorpion Solitaire is 1.1, released on 17/02/06.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8c9530afd34a309a88ff877f5fc83d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SYSTEM]\n",
      "TEXT: You are a precise analyzer of factual claims. Your task is to evaluate if claims made in a given 'response' are supported by a provided 'context', identify claims unique to the context, and detail your reasoning meticulously.\n",
      "\n",
      "**CORE TASK:**\n",
      "1.  Strictly identify atomic claims made *only* within the `response` text.\n",
      "2.  Strictly identify atomic claims made *only* within the `context` text (i.e., not mentioned in the `response`).\n",
      "3.  Verify if the claims from the `response` are supported by the `context`.\n",
      "4.  Outline your step-by-step reasoning process clearly separating response analysis from context analysis.\n",
      "\n",
      "**Follow these steps sequentially and precisely:**\n",
      "\n",
      "1.  **Analyze Response ONLY:** Read *only* the `response` text. Break it down into the smallest possible distinct, atomic factual statements that are *directly stated*. List these identified statements internally. This forms the candidate pool for `response_claims`.\n",
      "2.  **Finalize `response_claims`:** Review the statements identified in Step 1. Ensure they are truly atomic and directly from the `response`. Populate the `response_claims` list in the JSON with these finalized claims. **Do not proceed until this step is complete and accurate.**\n",
      "3.  **Analyze Context ONLY:** Now, read the `context` text. Identify all distinct, atomic factual claims within it. List these internally.\n",
      "4.  **Identify `context_only_claims`:** Compare the full list of context claims (from Step 3) with the finalized `response_claims` list (from Step 2). Create the `context_only_claims` list by including only those context claims that are *NOT* present in `response_claims`.\n",
      "5.  **Verify `response_claims` against Context:** For *each* claim in the `response_claims` list (from Step 2), check if the `context` text fully supports it.\n",
      "    *   \"Supported\" means *all* information in the response claim is explicitly stated or directly inferable from the context.\n",
      "    *   \"Unsupported\" means the context is silent, contradicts, or only partially supports the response claim.\n",
      "    *   Populate `supported_response_claims` and `unsupported_response_claims` based *only* on this verification of `response_claims`.\n",
      "6.  **Generate Justifications for `response_claims`:** For *each* claim in `response_claims`, write a brief justification explaining *why* it is supported or unsupported, referencing the context comparison. Populate the `justifications` dictionary. **CRITICAL: Keys in this dictionary MUST exactly match the strings in the `response_claims` list.**\n",
      "7.  **Count `response_claims`:** Count the total number of claims in the `response_claims` list (`num_total_response_claims`). Count the number of claims in the `supported_response_claims` list (`num_supported_response_claims`). Prepare the justification strings for these counts.\n",
      "8.  **Outline Reasoning (Chain of Thought):** Describe the *exact* steps you took, emphasizing the separation of response analysis (Steps 1-2) from context analysis and comparison (Steps 3-7). Detail *what* claims were identified in the response *before* mentioning context analysis. Explain the verification process for *each* response claim. Populate the `chain_of_thought` field.\n",
      "9.  **Format Output:** Assemble the final JSON object ensuring all fields are present and in the specified order.\n",
      "\n",
      "**CRITICAL INSTRUCTIONS:**\n",
      "*   `response_claims` MUST contain ONLY atomic statements directly extracted from the `response`. Absolutely NO information from the `context` should influence this list.\n",
      "*   `context_only_claims` MUST contain ONLY atomic statements found in the `context` but demonstrably NOT in the `response`.\n",
      "*   Verification, justifications, counts, and count justifications apply ONLY to `response_claims`.\n",
      "*   Keys in the `justifications` dictionary MUST be exact string matches of claims found in the `response_claims` list.\n",
      "\n",
      "Input will contain a 'response' (text to analyze) and 'context' (reference information).\n",
      "\n",
      "Format your output as a single, valid JSON object with the following structure (Pay attention to the order):\n",
      "{\n",
      "  \"response\": \"the original response text\",\n",
      "  \"context\": \"the provided context\",\n",
      "  \"chain_of_thought\": \"Detailed step-by-step reasoning, explicitly separating response analysis from context analysis and verification.\",\n",
      "  \"response_claims\": [\n",
      "    \"atomic claim 1 extracted ONLY from response\",\n",
      "    // ... more claims solely from response\n",
      "  ],\n",
      "  \"context_only_claims\": [\n",
      "    \"claim A found ONLY in context, not in response\",\n",
      "    // ... more claims solely from context\n",
      "  ],\n",
      "  \"supported_response_claims\": [\n",
      "    \"response claim 1 verified against context\",\n",
      "    // ... more supported response claims\n",
      "  ],\n",
      "  \"unsupported_response_claims\": [\n",
      "    \"response claim 2 verified against context\",\n",
      "    // ... more unsupported response claims\n",
      "  ],\n",
      "  \"justifications\": {\n",
      "    // Keys MUST be claims from the 'response_claims' list\n",
      "    \"atomic claim 1 extracted ONLY from response\": \"Justification based on context\",\n",
      "    \"atomic claim 2 extracted ONLY from response\": \"Justification based on context (e.g., 'not in context', 'contradicted by context')\"\n",
      "    // ... justifications ONLY for response_claims\n",
      "  },\n",
      "  \"num_supported_response_claims_justification\": \"Explanation of how the supported count for response claims was derived.\",\n",
      "  \"num_supported_response_claims\": 1, // Integer count of supported response claims\n",
      "  \"num_total_response_claims_justification\": \"Explanation of how the total count for response claims was derived.\",\n",
      "  \"num_total_response_claims\": 2 // Integer count of total claims identified ONLY in the response\n",
      "}\n",
      "\n",
      "Be thorough. Your entire output must be ONLY the single JSON object, starting with { and ending with }. Do not add any explanations, notes, or other text before or after the JSON object.\n",
      "\n",
      "**EXAMPLES:** Pay close attention to the detailed `chain_of_thought`, the strict separation of claim sources in `response_claims` vs `context_only_claims`, the exact matching required for `justifications` keys, and the field order.\n",
      "\n",
      "input: {\n",
      "  \"response\": \"Marie Curie was a Polish-born physicist and chemist. She was the first woman to win a Nobel Prize.\",\n",
      "  \"context\": \"Marie SkÅ‚odowska Curie (7 November 1867 â€“ 4 July 1934) was a Polish and naturalized-French physicist and chemist who conducted pioneering research on radioactivity. She was the first woman to win a Nobel Prize, the first person to win Nobel Prizes in two scientific fields, and the only person to win Nobel Prizes in multiple scientific fields.\"\n",
      "}\n",
      "\n",
      "output: {\n",
      "  \"response\": \"Marie Curie was a Polish-born physicist and chemist. She was the first woman to win a Nobel Prize.\",\n",
      "  \"context\": \"Marie SkÅ‚odowska Curie (7 November 1867 â€“ 4 July 1934) was a Polish and naturalized-French physicist and chemist who conducted pioneering research on radioactivity. She was the first woman to win a Nobel Prize, the first person to win Nobel Prizes in two scientific fields, and the only person to win Nobel Prizes in multiple scientific fields.\",\n",
      "  \"chain_of_thought\": \"1. Analyzed Response ONLY: Identified two atomic claims directly stated: 'Marie Curie was a Polish-born physicist and chemist' and 'Marie Curie was the first woman to win a Nobel Prize'. 2. Finalized `response_claims` with these two claims. 3. Analyzed Context: Identified claims about full name, dates, French naturalization, radioactivity research, winning Nobel in two fields, being the only person to win in multiple fields. 4. Identified `context_only_claims`: Compared context claims (Step 3) to response claims (Step 2). Found claims about full name, dates, French naturalization, radioactivity, winning in two fields, and being the only person to win multiple are unique to the context. 5. Verified `response_claims`: Claim 1 ('Polish-born physicist and chemist') is supported by context. Claim 2 ('first woman to win Nobel Prize') is supported by context. 6. Generated Justifications: Created justifications for the two response claims based on context verification. Keys exactly match `response_claims`. 7. Counted `response_claims`: Total=2, Supported=2. Prepared count justifications. 8. Formatted JSON output.\",\n",
      "  \"response_claims\": [\n",
      "    \"Marie Curie was a Polish-born physicist and chemist\",\n",
      "    \"Marie Curie was the first woman to win a Nobel Prize\"\n",
      "  ],\n",
      "  \"context_only_claims\": [\n",
      "    \"Marie Curie's full name was Marie SkÅ‚odowska Curie\",\n",
      "    \"Marie Curie lived from 7 November 1867 to 4 July 1934\",\n",
      "    \"Marie Curie was also naturalized-French\",\n",
      "    \"Marie Curie conducted pioneering research on radioactivity\",\n",
      "    \"Marie Curie was the first person to win Nobel Prizes in two scientific fields\",\n",
      "    \"Marie Curie was the only person to win Nobel Prizes in multiple scientific fields\"\n",
      "  ],\n",
      "  \"supported_response_claims\": [\n",
      "    \"Marie Curie was a Polish-born physicist and chemist\",\n",
      "    \"Marie Curie was the first woman to win a Nobel Prize\"\n",
      "  ],\n",
      "  \"unsupported_response_claims\": [],\n",
      "  \"justifications\": {\n",
      "    \"Marie Curie was a Polish-born physicist and chemist\": \"in context - context confirms Polish physicist and chemist\",\n",
      "    \"Marie Curie was the first woman to win a Nobel Prize\": \"in context\"\n",
      "  },\n",
      "  \"num_supported_response_claims_justification\": \"Derived by comparing the 2 claims identified strictly from the response against the context. Both were found to be fully supported.\",\n",
      "  \"num_supported_response_claims\": 2,\n",
      "  \"num_total_response_claims_justification\": \"Derived by breaking down the response ONLY into distinct factual statements. 2 atomic claims were identified.\",\n",
      "  \"num_total_response_claims\": 2\n",
      "}\n",
      "\n",
      "input: {\n",
      "  \"response\": \"The Great Wall of China is 13,171 miles long and visible from space.\",\n",
      "  \"context\": \"The Great Wall of China is an ancient series of walls and fortifications spanning approximately 13,171 miles (21,196 kilometers). Construction began as early as the 7th century BCE. It attracts millions of tourists each year.\"\n",
      "}\n",
      "\n",
      "output: {\n",
      "  \"response\": \"The Great Wall of China is 13,171 miles long and visible from space.\",\n",
      "  \"context\": \"The Great Wall of China is an ancient series of walls and fortifications spanning approximately 13,171 miles (21,196 kilometers). Construction began as early as the 7th century BCE. It attracts millions of tourists each year.\",\n",
      "  \"chain_of_thought\": \"1. Analyzed Response ONLY: Identified two atomic claims directly stated: 'The Great Wall of China is 13,171 miles long' and 'The Great Wall of China is visible from space'. 2. Finalized `response_claims` with these two claims. 3. Analyzed Context: Identified claims about being ancient walls/fortifications, km length, construction start date, attracting millions of tourists. 4. Identified `context_only_claims`: Compared context claims (Step 3) to response claims (Step 2). Found claims about ancient walls, km length, construction date, and tourism are unique to the context. 5. Verified `response_claims`: Claim 1 ('13,171 miles long') is supported by context ('approximately 13,171 miles'). Claim 2 ('visible from space') is unsupported as context does not mention it. 6. Generated Justifications: Created justifications for the two response claims based on context verification. Keys exactly match `response_claims`. 7. Counted `response_claims`: Total=2, Supported=1. Prepared count justifications. 8. Formatted JSON output.\",\n",
      "  \"response_claims\": [\n",
      "    \"The Great Wall of China is 13,171 miles long\",\n",
      "    \"The Great Wall of China is visible from space\"\n",
      "  ],\n",
      "  \"context_only_claims\": [\n",
      "    \"The Great Wall of China is an ancient series of walls and fortifications\",\n",
      "    \"The Great Wall of China spans 21,196 kilometers\",\n",
      "    \"Construction began as early as the 7th century BCE\",\n",
      "    \"The Great Wall attracts millions of tourists each year\"\n",
      "  ],\n",
      "  \"supported_response_claims\": [\n",
      "    \"The Great Wall of China is 13,171 miles long\"\n",
      "  ],\n",
      "  \"unsupported_response_claims\": [\n",
      "    \"The Great Wall of China is visible from space\"\n",
      "  ],\n",
      "  \"justifications\": {\n",
      "    \"The Great Wall of China is 13,171 miles long\": \"in context - context states length is approximately 13,171 miles\",\n",
      "    \"The Great Wall of China is visible from space\": \"not in context - context does not mention visibility from space\"\n",
      "  },\n",
      "  \"num_supported_response_claims_justification\": \"Derived by comparing the 2 claims identified strictly from the response against the context. 1 claim was found to be fully supported.\",\n",
      "  \"num_supported_response_claims\": 1,\n",
      "  \"num_total_response_claims_justification\": \"Derived by breaking down the response ONLY into distinct factual statements. 2 atomic claims were identified.\",\n",
      "  \"num_total_response_claims\": 2\n",
      "}\n",
      "\n",
      "Now perform the above instruction with the following input:\n",
      "\n",
      "input: {\n",
      "    \"response\": \"According to the Softonic website, the latest version of Scorpion Solitaire is 1.1, released on 17/02/06.\",\n",
      "    \"context\": \"Scorpion Solitaire\\n- User\\n- -\\n-\\n- Insufficient votes\\n- Softonic\\n- 6\\n- Not bad\\n- Not bad\\n- Your rating:\\n-\\nYour rating has been saved\\nOops, something's gone wrong. Try again.\\n- License:\\n- Free\\n- Language:\\n- OS:\\n-\\n- Latest version:\\n- 1.1 17/02/06\\n- Last month's downloads:\\n- 41\\n- -...\\n- PocketBalone\\n- Billiard Master\\n- Chess\\n- ...\\n- 21\\nScorpion Solitaire\\nSoftonic - Top Downloads\\nTop Downloads\\n- Pocket Uno\\nPlay the classic card game on your Pocket PC\\n- Multiplayer Championship Poker -...\\n- PocketBalone\\n- Billiard Master\\n- Trivial Pursuit\\n- ...\\n- 21\\nScorpion Solitaire.\"\n",
      "}\n",
      "\n",
      "Respond only with a valid JSON object that complies with the specified schema.\n",
      "**CRITICAL: Your entire response must be ONLY the single JSON object, starting with { and ending with }.\n",
      "Do not add any explanations, notes, or other text before or after the JSON object.**\n",
      "\n",
      "output:\n",
      "{\n",
      "  \"response\": \"According to the Softonic website, the latest version of Scorpion Solitaire is 1.1, released on 17/02/06.\",\n",
      "  \"context\": \"Scorpion Solitaire\\n- User\\n- -\\n-\\n- Insufficient votes\\n- Softonic\\n- 6\\n- Not bad\\n- Not bad\\n- Your rating:\\n-\\nYour rating has been saved\\nOops, something's gone wrong. Try again.\\n- License:\\n- Free\\n- Language:\\n- OS:\\n-\\n- Latest version:\\n- 1.1 17/02/06\\n- Last month's downloads:\\n- 41\\n- -...\\n- PocketBalone\\n- Billiard Master\\n- Chess\\n-...\\n- 21\\nScorpion Solitaire\\nSoftonic - Top Downloads\\nTop Downloads\\n- Pocket Uno\\nPlay the classic card game on your Pocket PC\\n- Multiplayer Championship Poker -...\\n- PocketBalone\\n- Billiard Master\\n- Trivial Pursuit\\n-...\\n- 21\\nScorpion Solitaire.\",\n",
      "  \"chain_of_thought\": \"1. Analyzed Response ONLY: Identified one atomic claim directly stated: 'the latest version of Scorpion Solitaire is 1.1, released on 17/02/06'. 2. Finalized `response_claims` with this one claim. 3. Analyzed Context: Identified claims about user interface, insufficient votes, Softonic rating, free license, latest version, and downloads. 4. Identified `context_only_claims`: Compared context claims (Step 3) to response claims (Step 2). Found claims about user interface, insufficient votes, Softonic rating, free license, latest version, and downloads are unique to the context. 5. Verified `response_claims`: Claim 1 ('the latest version of Scorpion Solitaire is 1.1, released on 17/02/06') is supported by context. 6. Generated Justifications: Created justification for the response claim based on context verification. Keys exactly match `response_claims`. 7. Counted `response_claims`: Total=1, Supported=1. Prepared count justifications. 8. Formatted JSON output.\",\n",
      "  \"response_claims\": [\n",
      "    \"the latest version of Scorpion Solitaire is 1.1, released on 17/02/06\"\n",
      "  ],\n",
      "  \"context_only_claims\": [\n",
      "    \"Scorpion Solitaire\\n- User\\n- -\\n-\\n- Insufficient votes\\n- Softonic\\n- 6\\n- Not bad\\n- Not bad\\n- Your rating:\\n-\\nYour rating has been saved\\nOops, something's gone wrong. Try again.\\n- License:\\n- Free\\n- Language:\\n- OS:\\n-\\n- Latest version:\\n- 1.1 17/02/06\\n- Last month's downloads:\\n- 41\\n- -...\\n- PocketBalone\\n- Billiard Master\\n- Chess\\n-...\\n- 21\\nScorpion Solitaire\\nSoftonic - Top Downloads\\nTop Downloads\\n- Pocket Uno\\nPlay the classic card game on your Pocket PC\\n- Multiplayer Championship Poker -...\\n- PocketBalone\\n- Billiard Master\\n- Trivial Pursuit\\n-...\\n- 21\\nScorpion Solitaire.\"\n",
      "  ],\n",
      "  \"supported_response_claims\": [\n",
      "    \"the latest version of Scorpion Solitaire is 1.1, released on 17/02/06\"\n",
      "  ],\n",
      "  \"unsupported_response_claims\": [],\n",
      "  \"justifications\": {\n",
      "    \"the latest version of Scorpion Solitaire is 1.1, released on 17/02/06\": \"in context - context states the latest version and release date\"\n",
      "  },\n",
      "  \"num_supported_response_claims_justification\": \"Derived by comparing the 1 claim identified strictly from the response against the context. 1 claim was found to be fully supported.\",\n",
      "  \"num_supported_response_claims\": 1,\n",
      "  \"num_total_response_claims_justification\": \"Derived by breaking down the response ONLY into distinct factual statements. 1 atomic claim was identified.\",\n",
      "  \"num_total_response_claims\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# 4.4 Test Faithfulness Prompt and interactions\n",
    "# --------------------------------------------------\n",
    "\n",
    "# RES IS OUTPUT FROM CHAIN HERE\n",
    "# --- Extract the part after \"Helpful Answer:\" ---\n",
    "result_string = res.get(\"result\", \"\") # Get the result string, default to empty if key missing\n",
    "marker = \"Helpful Answer:\"\n",
    "\n",
    "# Use partition to split the string based on the first occurrence of the marker\n",
    "before_marker, found_marker, after_marker = result_string.partition(marker)\n",
    "\n",
    "# Check if the marker was actually found\n",
    "if found_marker:\n",
    "    # If found, take the part after it and strip leading/trailing whitespace\n",
    "    extracted_answer = after_marker.strip()\n",
    "else:\n",
    "    # If marker wasn't found, the whole result might be the answer,\n",
    "    # or something went wrong. Handle as appropriate.\n",
    "    # For now, let's assume the whole string is the fallback, but print a warning.\n",
    "    print(f\"\\nWarning: Marker '{marker}' not found in result. Using full result string.\")\n",
    "    extracted_answer = result_string.strip() # Use the whole string\n",
    "\n",
    "print(\"\\nEXTRACTED ANSWER:\")\n",
    "print(extracted_answer)\n",
    "\n",
    "##############################\n",
    "response = extracted_answer\n",
    "context = test_data[\"context\"][0]\n",
    "# print(context)\n",
    "\n",
    "################################################################################\n",
    "# JUDGE LLM????\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tokenizer_id = model_id # Usually the same\n",
    "\n",
    "# Load tokenizer explicitly first to set pad token\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "\n",
    "# Make pipeline\n",
    "hf_gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    tokenizer=tokenizer_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device=0,\n",
    "    max_new_tokens=1024,\n",
    "    batch_size=20,\n",
    "    temperature=0.01,\n",
    "    do_sample=True,\n",
    ")\n",
    "hf_gen.tokenizer.pad_token = hf_gen.tokenizer.eos_token\n",
    "\n",
    "# Wrap it in LangChainâ€™s LLM interface\n",
    "llm_judge = HuggingFacePipeline(pipeline=hf_gen, model_kwargs={\"stop\": [\"}\\n```\", \"}\\n\\n\", \"}\\nHere is\"]})\n",
    "################################################################################\n",
    "\n",
    "modified_faithfulness_prompt = create_faithfulness_prompt(response, context)\n",
    "example_faith = llm_judge(modified_faithfulness_prompt, return_full_text=False)\n",
    "print(example_faith)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d42478c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# 4.6 mess with answer relevancy\n",
    "# ----------------------------------------------------\n",
    "def create_answer_relevancy_prompt(question: str, answer: str, context: str, num_questions: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Generates the Answer Relevancy question generation prompt.\n",
    "\n",
    "    Args:\n",
    "        question: The original question asked.\n",
    "        answer: The answer generated by the LLM.\n",
    "        context: The context provided to the LLM (can inform question style).\n",
    "        num_questions: The number of questions to generate (default 3).\n",
    "\n",
    "    Returns:\n",
    "        The complete prompt string ready for the LLM.\n",
    "    \"\"\"\n",
    "    # Escape inputs for safe JSON embedding\n",
    "    question_escaped = json.dumps(question)\n",
    "    answer_escaped = json.dumps(answer)\n",
    "    context_escaped = json.dumps(context) # Context might still be useful for style/scope\n",
    "\n",
    "    # Define the prompt template\n",
    "    prompt = f\"\"\"\n",
    "[SYSTEM]\n",
    "TEXT: You are an expert analytical tool. Your task is to generate plausible questions based on a given answer, considering the original question and context. The goal is to create questions that the provided answer directly and comprehensively addresses.\n",
    "\n",
    "**CORE TASK:**\n",
    "Generate {num_questions} distinct questions for which the provided `answer` is a relevant and informative response.\n",
    "\n",
    "**Follow these steps sequentially and precisely:**\n",
    "\n",
    "1.  **Analyze the `answer` ONLY:** Read the `answer` text carefully. Identify the key pieces of information, main points, and specific details presented in the answer. List these internally.\n",
    "2.  **Analyze the original `question`:** Understand the topic, scope, and intent of the original `question` that led to the `answer`.\n",
    "3.  **Analyze the `context` (Optional but Recommended):** Briefly review the `context` to understand the source material's style and scope, which might help in phrasing the generated questions appropriately. However, the generated questions MUST be answerable primarily by the provided `answer`.\n",
    "4.  **Generate {num_questions} Questions:** Based *only* on the key information identified in the `answer` (Step 1), formulate {num_questions} distinct questions.\n",
    "    *   Each generated question should be clearly and fully answerable using *only* the information present in the provided `answer`.\n",
    "    *   The questions should be natural-sounding and relevant to the topic suggested by the original `question` and the `answer`.\n",
    "    *   Avoid generating questions that require information *not* present in the `answer`.\n",
    "5.  **Outline Reasoning (Chain of Thought):** Describe the exact steps you took. Explain:\n",
    "    *   What key information you extracted from the `answer`.\n",
    "    *   How you considered the original `question`'s topic.\n",
    "    *   How each generated question specifically relates to and is answerable by the content of the `answer`.\n",
    "    *   Populate the `chain_of_thought` field with this detailed reasoning.\n",
    "6.  **Format Output:** Assemble the final JSON object ensuring all fields are present and in the specified order.\n",
    "\n",
    "**CRITICAL INSTRUCTIONS:**\n",
    "*   The `generated_questions` list MUST contain exactly {num_questions} distinct questions.\n",
    "*   Each generated question MUST be answerable using *only* the information explicitly present in the provided `answer`.\n",
    "*   The generated questions should reflect the core information conveyed by the `answer`.\n",
    "\n",
    "Input will contain the original 'question', the generated 'answer', and the 'context'.\n",
    "\n",
    "Format your output as a single, valid JSON object with the following structure (Pay attention to the order):\n",
    "{{\n",
    "  \"question\": \"the original question text\",\n",
    "  \"answer\": \"the provided answer text\",\n",
    "  \"context\": \"the provided context text\",\n",
    "  \"chain_of_thought\": \"Detailed step-by-step reasoning: 1. Analyzed answer for key info. 2. Considered original question topic. 3. (Optional) Considered context style. 4. Formulated N questions based *only* on answer content. 5. Verified questions are answerable by the answer.\",\n",
    "  \"generated_questions\": [\n",
    "    \"Generated question 1 based ONLY on the answer\",\n",
    "    \"Generated question 2 based ONLY on the answer\",\n",
    "    \"Generated question 3 based ONLY on the answer\"\n",
    "    // ... up to N questions\n",
    "  ],\n",
    "  \"num_generated_questions\": {num_questions} // Integer count of generated questions\n",
    "}}\n",
    "\n",
    "Be thorough. Your entire output must be ONLY the single JSON object, starting with {{ and ending with }}. Do not add any explanations, notes, or other text before or after the JSON object.\n",
    "\n",
    "**EXAMPLES:** Pay close attention to the detailed `chain_of_thought` and how the `generated_questions` strictly derive from the `answer`.\n",
    "\n",
    "input: {{\n",
    "  \"question\": \"Who was Marie Curie?\",\n",
    "  \"answer\": \"Marie Curie was a Polish-born physicist and chemist, famous for her pioneering research on radioactivity. She was the first woman to win a Nobel Prize.\",\n",
    "  \"context\": \"Marie SkÅ‚odowska Curie (7 November 1867 â€“ 4 July 1934) was a Polish and naturalized-French physicist and chemist who conducted pioneering research on radioactivity. She was the first woman to win a Nobel Prize, the first person to win Nobel Prizes in two scientific fields...\"\n",
    "}}\n",
    "\n",
    "output: {{\n",
    "  \"question\": \"Who was Marie Curie?\",\n",
    "  \"answer\": \"Marie Curie was a Polish-born physicist and chemist, famous for her pioneering research on radioactivity. She was the first woman to win a Nobel Prize.\",\n",
    "  \"context\": \"Marie SkÅ‚odowska Curie (7 November 1867 â€“ 4 July 1934) was a Polish and naturalized-French physicist and chemist who conducted pioneering research on radioactivity. She was the first woman to win a Nobel Prize, the first person to win Nobel Prizes in two scientific fields...\",\n",
    "  \"chain_of_thought\": \"1. Analyzed Answer: Key info includes Polish-born physicist/chemist, research on radioactivity, first woman Nobel winner. 2. Considered Original Question: Broad 'Who was...' question. 3. Considered Context: Confirms details, adds more specifics not in answer. 4. Formulated 3 questions based *only* on answer content: Focused on nationality/profession, research area, and Nobel achievement mentioned. 5. Verified questions are answerable by the answer.\",\n",
    "  \"generated_questions\": [\n",
    "    \"What was Marie Curie's nationality and profession?\",\n",
    "    \"What field did Marie Curie conduct pioneering research in?\",\n",
    "    \"What significant 'first' did Marie Curie achieve regarding the Nobel Prize?\"\n",
    "  ],\n",
    "  \"num_generated_questions\": 3\n",
    "}}\n",
    "\n",
    "input: {{\n",
    "  \"question\": \"What is the Great Wall of China?\",\n",
    "  \"answer\": \"The Great Wall of China is a very long ancient wall in China.\",\n",
    "  \"context\": \"The Great Wall of China is an ancient series of walls and fortifications spanning approximately 13,171 miles... Construction began as early as the 7th century BCE...\"\n",
    "}}\n",
    "\n",
    "output: {{\n",
    "  \"question\": \"What is the Great Wall of China?\",\n",
    "  \"answer\": \"The Great Wall of China is a very long ancient wall in China.\",\n",
    "  \"context\": \"The Great Wall of China is an ancient series of walls and fortifications spanning approximately 13,171 miles... Construction began as early as the 7th century BCE...\",\n",
    "  \"chain_of_thought\": \"1. Analyzed Answer: Key info is 'very long', 'ancient wall', 'in China'. 2. Considered Original Question: 'What is...' question. 3. Considered Context: Provides specifics like length, type, date. 4. Formulated 3 questions based *only* on the limited answer content: Focused on length description, age description, and location. 5. Verified questions are answerable by the answer.\",\n",
    "  \"generated_questions\": [\n",
    "    \"How is the length of the Great Wall of China described?\",\n",
    "    \"What is stated about the age of the Great Wall of China?\",\n",
    "    \"Where is the Great Wall located?\"\n",
    "  ],\n",
    "  \"num_generated_questions\": 3\n",
    "}}\n",
    "\n",
    "Now perform the above instruction with the following input:\n",
    "\n",
    "input: {{\n",
    "    \"question\": {question_escaped},\n",
    "    \"answer\": {answer_escaped},\n",
    "    \"context\": {context_escaped}\n",
    "}}\n",
    "\n",
    "Respond only with a valid JSON object that complies with the specified schema.\n",
    "**CRITICAL: Your entire response must be ONLY the single JSON object, starting with {{ and ending with }}.\n",
    "Do not add any explanations, notes, or other text before or after the JSON object.**\n",
    "\n",
    "output:\n",
    "\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "92a6009e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "\n",
    "# response = extracted_answer\n",
    "context = test_data[\"context\"][0]\n",
    "\n",
    "# Use one of the previous test cases or define new ones\n",
    "original_question = test_data[\"question\"][0]\n",
    "generated_answer = extracted_answer\n",
    "context_info = context\n",
    "\n",
    "# Generate the prompt\n",
    "relevancy_prompt = create_answer_relevancy_prompt(original_question, generated_answer, context_info, num_questions=3)\n",
    "\n",
    "# Print the generated prompt (optional, for verification)\n",
    "# print(\"\\n--- Generated Prompt for Answer Relevancy ---\")\n",
    "# print(relevancy_prompt)\n",
    "# print(\"--- End of Prompt ---\")\n",
    "\n",
    "# Now you would pass 'relevancy_prompt' to your LLM\n",
    "relevancy_out = llm_judge(relevancy_prompt, return_full_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bf8fd9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is the version number of Scorpion Solitaire?', 'When was the latest version of Scorpion Solitaire released?', 'Who is the source of the latest version information for Scorpion Solitaire?']\n"
     ]
    }
   ],
   "source": [
    "# print(relevancy_out)\n",
    "# Parse the JSON output from the LLM\n",
    "generated_questions = extract_json_from_output(relevancy_out)\n",
    "print(generated_questions['generated_questions'])\n",
    "\n",
    "# Then proceed with embedding comparison..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58ce4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation for 19 samples with 19 workers...\n",
      "Starting QA for index: 0\n",
      "Starting QA for index: 1\n",
      "Starting QA for index: 2\n",
      "Starting QA for index: 3\n",
      "Starting QA for index: 4\n",
      "Starting QA for index: 5\n",
      "Starting QA for index: 6\n",
      "Starting QA for index: 7\n",
      "Starting QA for index: 8\n",
      "Starting QA for index: 9\n",
      "Starting QA for index: 10\n",
      "Starting QA for index: 11\n",
      "Starting QA for index: 12\n",
      "Starting QA for index: 13\n",
      "Starting QA for index: 14\n",
      "Starting QA for index: 15\n",
      "Starting QA for index: 16\n",
      "Starting QA for index: 17\n",
      "Starting QA for index: 18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f010d8fac54ba99fdb386f62d269fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Samples:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished QA for index: 12\n",
      "Calculating metrics for index: 12\n",
      "Finished metrics for index: 12\n",
      "Finished QA for index: 2\n",
      "Calculating metrics for index: 2\n",
      "Finished metrics for index: 2\n",
      "Finished QA for index: 9\n",
      "Calculating metrics for index: 9\n",
      "Finished metrics for index: 9\n",
      "Finished QA for index: 10\n",
      "Calculating metrics for index: 10\n",
      "Finished metrics for index: 10\n",
      "Finished QA for index: 18\n",
      "Calculating metrics for index: 18\n",
      "Finished metrics for index: 18\n",
      "Finished QA for index: 11\n",
      "Calculating metrics for index: 11\n",
      "Finished metrics for index: 11\n",
      "Finished QA for index: 7\n",
      "Calculating metrics for index: 7\n",
      "Finished metrics for index: 7\n",
      "Finished QA for index: 17\n",
      "Calculating metrics for index: 17\n",
      "Finished metrics for index: 17\n",
      "Finished QA for index: 1\n",
      "Calculating metrics for index: 1\n",
      "Finished metrics for index: 1\n",
      "Finished QA for index: 14\n",
      "Calculating metrics for index: 14\n",
      "Finished metrics for index: 14\n",
      "Finished QA for index: 5\n",
      "Calculating metrics for index: 5\n",
      "Finished metrics for index: 5\n",
      "Finished QA for index: 8\n",
      "Calculating metrics for index: 8\n",
      "Finished metrics for index: 8\n",
      "Finished QA for index: 13\n",
      "Calculating metrics for index: 13\n",
      "Finished metrics for index: 13\n",
      "Finished QA for index: 0\n",
      "Calculating metrics for index: 0\n",
      "Finished metrics for index: 0\n",
      "Finished QA for index: 15\n",
      "Calculating metrics for index: 15\n",
      "Finished metrics for index: 15\n",
      "Finished QA for index: 6\n",
      "Calculating metrics for index: 6\n",
      "Finished metrics for index: 6\n",
      "Finished QA for index: 4\n",
      "Calculating metrics for index: 4\n",
      "Finished metrics for index: 4\n",
      "Finished QA for index: 3\n",
      "Calculating metrics for index: 3\n",
      "Finished metrics for index: 3\n",
      "Finished QA for index: 16\n",
      "Calculating metrics for index: 16\n",
      "Finished metrics for index: 16\n",
      "\n",
      "--- Evaluation Complete ---\n",
      "\n",
      "Processed 19 samples successfully.\n",
      "Average Context Precision: 1.0000\n",
      "Average Context Recall: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import concurrent.futures\n",
    "from tqdm.auto import tqdm # Progress bar library\n",
    "# -------------------------------------\n",
    "# 5. Scale up!!!!\n",
    "# -------------------------------------\n",
    "\n",
    "# --- Worker Function ---\n",
    "# This function processes a single entry from the test data\n",
    "def process_entry(args):\n",
    "    \"\"\"\n",
    "    Processes a single test data entry: runs QA, prepares sample, calculates RAGAS metrics.\n",
    "    Args:\n",
    "        args (tuple): A tuple containing (index, entry_dict).\n",
    "    Returns:\n",
    "        dict: A dictionary containing results or error information.\n",
    "    \"\"\"\n",
    "    index, entry = args\n",
    "    try:\n",
    "        # 1. Run QA Chain (Synchronous I/O bound task)\n",
    "        print(f\"Starting QA for index: {index}\") # Optional: Track start\n",
    "        res = qa_chain({\"query\": entry[\"question\"]})\n",
    "        print(f\"Finished QA for index: {index}\") # Optional: Track end\n",
    "\n",
    "        # 2. Extract Contexts\n",
    "        retrieved_contexts = [doc.page_content for doc in res.get(\"source_documents\", [])]\n",
    "\n",
    "        # Ensure reference_contexts is a list of strings\n",
    "        reference_contexts_raw = entry.get(\"context\", []) # Get reference context(s)\n",
    "        if isinstance(reference_contexts_raw, str):\n",
    "            reference_contexts = [reference_contexts_raw]\n",
    "        elif isinstance(reference_contexts_raw, list):\n",
    "            reference_contexts = reference_contexts_raw\n",
    "        else:\n",
    "            print(f\"Warning: Unexpected type for reference context at index {index}. Treating as empty list.\")\n",
    "            reference_contexts = []\n",
    "\n",
    "        # 3. Prepare RAGAS Sample\n",
    "        # Adjust fields based on the metrics you are using.\n",
    "        # Here assuming only context metrics are needed.\n",
    "        sample = SingleTurnSample(\n",
    "            retrieved_contexts=retrieved_contexts,\n",
    "            reference_contexts=reference_contexts\n",
    "            # You might need question, answer, etc. for other RAGAS metrics\n",
    "            # question=entry[\"question\"],\n",
    "            # answer=res.get(\"result\", \"\")\n",
    "        )\n",
    "\n",
    "        # 4. Calculate Metrics (Run async metrics concurrently)\n",
    "        async def calculate_metrics_async(sample_to_score):\n",
    "            \"\"\"Helper async function to gather metric scores.\"\"\"\n",
    "            print(f\"Calculating metrics for index: {index}\") # Optional\n",
    "            # Add more metric tasks here if needed\n",
    "            precision_task = context_precision.single_turn_ascore(sample_to_score)\n",
    "            recall_task = context_recall.single_turn_ascore(sample_to_score)\n",
    "\n",
    "            # Run concurrently and wait for results\n",
    "            precision_score, recall_score = await asyncio.gather(\n",
    "                precision_task,\n",
    "                recall_task\n",
    "                # Add other awaited tasks here\n",
    "            )\n",
    "            print(f\"Finished metrics for index: {index}\") # Optional\n",
    "            return precision_score, recall_score\n",
    "\n",
    "        # Use asyncio.run to execute the async helper from this sync thread\n",
    "        precision, recall = asyncio.run(calculate_metrics_async(sample))\n",
    "\n",
    "        # 5. Prepare result dictionary\n",
    "        result_data = {\n",
    "            \"index\": index,\n",
    "            \"question\": entry[\"question\"],\n",
    "            \"retrieved_contexts\": retrieved_contexts,\n",
    "            \"reference_contexts\": reference_contexts,\n",
    "            \"context_precision\": precision,\n",
    "            \"context_recall\": recall,\n",
    "            \"raw_qa_response\": res # Optional: store full response\n",
    "        }\n",
    "        # Optional: print immediate results per sample\n",
    "        # print(f\"METRICS FOR INDEX {index}: CP={precision:.4f}, CR={recall:.4f}\")\n",
    "        return result_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing index {index} - Question: {entry.get('question', 'N/A')}: {e}\")\n",
    "        # Optionally log the full traceback\n",
    "        # import traceback\n",
    "        # traceback.print_exc()\n",
    "        return {\n",
    "            \"index\": index,\n",
    "            \"question\": entry.get('question', 'N/A'),\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "        \n",
    "# --- Main Execution ---\n",
    "all_results = []\n",
    "# Prepare arguments for mapping (index, entry) tuples\n",
    "tasks = list(enumerate(test_data)) # Creates [(0, test_data[0]), (1, test_data[1]), ...]\n",
    "\n",
    "# Use ThreadPoolExecutor for parallel execution\n",
    "num_workers = 20\n",
    "print(f\"Starting evaluation for {len(tasks)} samples with {num_workers} workers...\")\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "    # Use executor.map to process tasks in parallel.\n",
    "    # Results will be yielded in the order tasks were submitted (or close to it).\n",
    "    # Wrap with tqdm for a progress bar.\n",
    "    results_iterator = list(tqdm(executor.map(process_entry, tasks), total=len(tasks), desc=\"Evaluating Samples\"))\n",
    "\n",
    "all_results = results_iterator # list() consumes the iterator and gathers all results\n",
    "\n",
    "print(\"\\n--- Evaluation Complete ---\")\n",
    "\n",
    "# --- Post-Processing (Example) ---\n",
    "valid_results = [r for r in all_results if 'error' not in r]\n",
    "errors = [r for r in all_results if 'error' in r]\n",
    "\n",
    "if valid_results:\n",
    "    avg_precision = sum(r['context_precision'] for r in valid_results) / len(valid_results)\n",
    "    avg_recall = sum(r['context_recall'] for r in valid_results) / len(valid_results)\n",
    "    print(f\"\\nProcessed {len(valid_results)} samples successfully.\")\n",
    "    print(f\"Average Context Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average Context Recall: {avg_recall:.4f}\")\n",
    "else:\n",
    "    print(\"No samples processed successfully.\")\n",
    "\n",
    "if errors:\n",
    "    print(f\"\\nEncountered {len(errors)} errors during processing.\")\n",
    "    # Optionally print details of errors\n",
    "    # for error_result in errors:\n",
    "    #     print(f\"  Index {error_result['index']}: {error_result['error']}\")\n",
    "\n",
    "# Now `all_results` contains a list of dictionaries, each holding the\n",
    "# computed metrics and other info for one entry from test_data, or error details.\n",
    "# print(all_results[0]) # Example: Inspect the first result\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
