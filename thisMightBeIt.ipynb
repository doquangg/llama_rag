{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "498c999c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# --- Add near the beginning ---\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Using device: cuda:0\")\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    print(\"CUDA not available. Using device: cpu\")\n",
    "    device = \"cpu\"\n",
    "# --- Use 'device' variable later ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6cf090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded test split with 19 documents.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load the dataset\n",
    "# -------------------------------\n",
    "# Assume ds is a dictionary-like dataset with splits \"train\" and \"test\".\n",
    "# We use only the test split.\n",
    "# Each instance in ds['test'] is assumed to have 'context', 'question', and 'answer'.\n",
    "# Replace this with your actual data loading as needed.\n",
    "\n",
    "# Load the dataset with test and train splits.\n",
    "ds = load_dataset(\"neural-bridge/rag-dataset-12000\")\n",
    "n = 20  # Number of documents to select (adjust as needed)\n",
    "\n",
    "# Define a function to compute the context length.\n",
    "def add_context_length(example):\n",
    "    # Calculate the length of the context (number of characters)\n",
    "    example[\"context_length\"] = len(example[\"context\"])\n",
    "    return example\n",
    "\n",
    "# Add the new 'context_length' field to each example in the test split.\n",
    "test_data = ds['test'].map(add_context_length)\n",
    "\n",
    "# Sort the test split by the new context_length field in ascending order (shortest contexts first).\n",
    "test_data = test_data.sort(\"context_length\")\n",
    "\n",
    "# Select the top n documents with the shortest contexts.\n",
    "test_data = test_data.select(range(n))\n",
    "print(f\"Loaded test split with {len(test_data)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f16a6504",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# ------------------------------------\n",
    "# 2. Prepare embeddings for retrieval\n",
    "# ------------------------------------\n",
    "embed_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name=embed_model_name)\n",
    "\n",
    "# Extract all passages in one flat list\n",
    "passages = []\n",
    "for ex in test_data:\n",
    "    passages.extend([ex[\"context\"]])\n",
    "    \n",
    "# print(passages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7453ed8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# -------------------------------------\n",
    "# 3. Build a FAISS vectorstore (in-memory)\n",
    "# -------------------------------------\n",
    "# vectorstore = FAISS.from_documents(documents=all_splits, embedding=hf_embeddings)\n",
    "vectorstore = FAISS.from_texts(texts=passages, embedding=hf_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3ab229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain.chains import RetrievalQA\n",
    "import torch\n",
    "\n",
    "# --------------------------------------\n",
    "# 3.1 Create text-gen pipeline\n",
    "# --------------------------------------\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer_id = model_id # Usually the same\n",
    "\n",
    "# Load tokenizer explicitly first to set pad token\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "\n",
    "# Make pipeline\n",
    "hf_gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    tokenizer=tokenizer_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device=0,\n",
    "    max_new_tokens=128,\n",
    "    batch_size=20,\n",
    "    do_sample=True,\n",
    ")\n",
    "hf_gen.tokenizer.pad_token = hf_gen.tokenizer.eos_token\n",
    "\n",
    "# Wrap it in LangChainâ€™s LLM interface\n",
    "llm = HuggingFacePipeline(pipeline=hf_gen, model_kwargs={\"stop\": [\"}\\n```\", \"}\\n\\n\", \"}\\nHere is\"]})\n",
    "\n",
    "# Wrap both in RAGAS wrappers (correct order now)\n",
    "hf_wrapped = LangchainLLMWrapper(llm)\n",
    "hf_embed_wrapped = LangchainEmbeddingsWrapper(hf_embeddings)\n",
    "\n",
    "# Build retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "# 4. Now create the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",                # or \"map_reduce\" / \"refine\" if you prefer\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0fdd6147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHOLE RESPONSE:\n",
      "{'query': 'What is the latest version of Scorpion Solitaire?', 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nScorpion Solitaire\\n- User\\n- -\\n-\\n- Insufficient votes\\n- Softonic\\n- 6\\n- Not bad\\n- Not bad\\n- Your rating:\\n-\\nYour rating has been saved\\nOops, something's gone wrong. Try again.\\n- License:\\n- Free\\n- Language:\\n- OS:\\n-\\n- Latest version:\\n- 1.1 17/02/06\\n- Last month's downloads:\\n- 41\\n- -...\\n- PocketBalone\\n- Billiard Master\\n- Chess\\n- ...\\n- 21\\nScorpion Solitaire\\nSoftonic - Top Downloads\\nTop Downloads\\n- Pocket Uno\\nPlay the classic card game on your Pocket PC\\n- Multiplayer Championship Poker -...\\n- PocketBalone\\n- Billiard Master\\n- Trivial Pursuit\\n- ...\\n- 21\\nScorpion Solitaire.\\n\\nQuestion: What is the latest version of Scorpion Solitaire?\\nHelpful Answer: According to the Softonic website, the latest version of Scorpion Solitaire is 1.1, which was released on 17/02/06.\", 'source_documents': [Document(metadata={}, page_content=\"Scorpion Solitaire\\n- User\\n- -\\n-\\n- Insufficient votes\\n- Softonic\\n- 6\\n- Not bad\\n- Not bad\\n- Your rating:\\n-\\nYour rating has been saved\\nOops, something's gone wrong. Try again.\\n- License:\\n- Free\\n- Language:\\n- OS:\\n-\\n- Latest version:\\n- 1.1 17/02/06\\n- Last month's downloads:\\n- 41\\n- -...\\n- PocketBalone\\n- Billiard Master\\n- Chess\\n- ...\\n- 21\\nScorpion Solitaire\\nSoftonic - Top Downloads\\nTop Downloads\\n- Pocket Uno\\nPlay the classic card game on your Pocket PC\\n- Multiplayer Championship Poker -...\\n- PocketBalone\\n- Billiard Master\\n- Trivial Pursuit\\n- ...\\n- 21\\nScorpion Solitaire.\")]}\n",
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Scorpion Solitaire\n",
      "- User\n",
      "- -\n",
      "-\n",
      "- Insufficient votes\n",
      "- Softonic\n",
      "- 6\n",
      "- Not bad\n",
      "- Not bad\n",
      "- Your rating:\n",
      "-\n",
      "Your rating has been saved\n",
      "Oops, something's gone wrong. Try again.\n",
      "- License:\n",
      "- Free\n",
      "- Language:\n",
      "- OS:\n",
      "-\n",
      "- Latest version:\n",
      "- 1.1 17/02/06\n",
      "- Last month's downloads:\n",
      "- 41\n",
      "- -...\n",
      "- PocketBalone\n",
      "- Billiard Master\n",
      "- Chess\n",
      "- ...\n",
      "- 21\n",
      "Scorpion Solitaire\n",
      "Softonic - Top Downloads\n",
      "Top Downloads\n",
      "- Pocket Uno\n",
      "Play the classic card game on your Pocket PC\n",
      "- Multiplayer Championship Poker -...\n",
      "- PocketBalone\n",
      "- Billiard Master\n",
      "- Trivial Pursuit\n",
      "- ...\n",
      "- 21\n",
      "Scorpion Solitaire.\n",
      "\n",
      "Question: What is the latest version of Scorpion Solitaire?\n",
      "Helpful Answer: According to the Softonic website, the latest version of Scorpion Solitaire is 1.1, which was released on 17/02/06.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# 3.2 Test it on the first user_input\n",
    "# --------------------------------------------------\n",
    "res = qa_chain({\"query\": test_data[\"question\"][0]})\n",
    "print(\"WHOLE RESPONSE:\")\n",
    "print(res)\n",
    "print(res[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d14fc7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RETREIVED CONTENT\n",
      "Scorpion Solitaire\n",
      "- User\n",
      "- -\n",
      "-\n",
      "- Insufficient votes\n",
      "- Softonic\n",
      "- 6\n",
      "- Not bad\n",
      "- Not bad\n",
      "- Your rating:\n",
      "-\n",
      "Your rating has been saved\n",
      "Oops, something's gone wrong. Try again.\n",
      "- License:\n",
      "- Free\n",
      "- Language:\n",
      "- OS:\n",
      "-\n",
      "- Latest version:\n",
      "- 1.1 17/02/06\n",
      "- Last month's downloads:\n",
      "- 41\n",
      "- -...\n",
      "- PocketBalone\n",
      "- Billiard Master\n",
      "- Chess\n",
      "- ...\n",
      "- 21\n",
      "Scorpion Solitaire\n",
      "Softonic - Top Downloads\n",
      "Top Downloads\n",
      "- Pocket Uno\n",
      "Play the classic card game on your Pocket PC\n",
      "- Multiplayer Championship Poker -...\n",
      "- PocketBalone\n",
      "- Billiard Master\n",
      "- Trivial Pursuit\n",
      "- ...\n",
      "- 21\n",
      "Scorpion Solitaire.\n",
      "SOURCE CONTENT\n",
      "Scorpion Solitaire\n",
      "- User\n",
      "- -\n",
      "-\n",
      "- Insufficient votes\n",
      "- Softonic\n",
      "- 6\n",
      "- Not bad\n",
      "- Not bad\n",
      "- Your rating:\n",
      "-\n",
      "Your rating has been saved\n",
      "Oops, something's gone wrong. Try again.\n",
      "- License:\n",
      "- Free\n",
      "- Language:\n",
      "- OS:\n",
      "-\n",
      "- Latest version:\n",
      "- 1.1 17/02/06\n",
      "- Last month's downloads:\n",
      "- 41\n",
      "- -...\n",
      "- PocketBalone\n",
      "- Billiard Master\n",
      "- Chess\n",
      "- ...\n",
      "- 21\n",
      "Scorpion Solitaire\n",
      "Softonic - Top Downloads\n",
      "Top Downloads\n",
      "- Pocket Uno\n",
      "Play the classic card game on your Pocket PC\n",
      "- Multiplayer Championship Poker -...\n",
      "- PocketBalone\n",
      "- Billiard Master\n",
      "- Trivial Pursuit\n",
      "- ...\n",
      "- 21\n",
      "Scorpion Solitaire.\n",
      "CONTEXT PRECISION\n",
      "0.9999999999\n",
      "CONTEXT RECALL\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import NonLLMContextPrecisionWithReference\n",
    "from ragas.metrics import NonLLMContextRecall\n",
    "from ragas import SingleTurnSample\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Experiment with evaluation\n",
    "# --------------------------------------------------\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Context Precision Test on the first question\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Instantiate an object, for some reason\n",
    "context_precision = NonLLMContextPrecisionWithReference()\n",
    "\n",
    "# THIS IS HOW YOU GET THE PAGE CONTENT FOR A RETRIEVED DOCUMENT, 0 HERE IS THE FIRST RESULT\n",
    "print(\"RETREIVED CONTENT\")\n",
    "print(res[\"source_documents\"][0].page_content) # this will need to be a list, in practice\n",
    "\n",
    "# THIS IS HOW YOU GET THE SOURCE CONTEXT, 0 HERE IS THE FIRST DOCUMENT\n",
    "print(\"SOURCE CONTENT\")\n",
    "print(test_data[\"context\"][0]) # this will need to be a list, in practice\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    retrieved_contexts=[res[\"source_documents\"][0].page_content], \n",
    "    reference_contexts=[test_data[\"context\"][0]]\n",
    ")\n",
    "print(\"CONTEXT PRECISION\")\n",
    "print(await context_precision.single_turn_ascore(sample))\n",
    "\n",
    "# HOLY SHIT IT WORKS!!!\n",
    "\n",
    "# -------------------------------\n",
    "# Context Recall Test\n",
    "# -------------------------------\n",
    "\n",
    "# stupid fucking object\n",
    "context_recall = NonLLMContextRecall()\n",
    "print(\"CONTEXT RECALL\")\n",
    "print(await context_recall.single_turn_ascore(sample))\n",
    "\n",
    "# I'm actually tony stark btw... I think we can do answer relevancy and faithfulness later??? \n",
    "# Right now, scale up these two...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58ce4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation for 19 samples with 19 workers...\n",
      "Starting QA for index: 0\n",
      "Starting QA for index: 1\n",
      "Starting QA for index: 2\n",
      "Starting QA for index: 3\n",
      "Starting QA for index: 4\n",
      "Starting QA for index: 5\n",
      "Starting QA for index: 6\n",
      "Starting QA for index: 7\n",
      "Starting QA for index: 8\n",
      "Starting QA for index: 9\n",
      "Starting QA for index: 10\n",
      "Starting QA for index: 11\n",
      "Starting QA for index: 12\n",
      "Starting QA for index: 13\n",
      "Starting QA for index: 14\n",
      "Starting QA for index: 15\n",
      "Starting QA for index: 16\n",
      "Starting QA for index: 17\n",
      "Starting QA for index: 18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f010d8fac54ba99fdb386f62d269fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Samples:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished QA for index: 12\n",
      "Calculating metrics for index: 12\n",
      "Finished metrics for index: 12\n",
      "Finished QA for index: 2\n",
      "Calculating metrics for index: 2\n",
      "Finished metrics for index: 2\n",
      "Finished QA for index: 9\n",
      "Calculating metrics for index: 9\n",
      "Finished metrics for index: 9\n",
      "Finished QA for index: 10\n",
      "Calculating metrics for index: 10\n",
      "Finished metrics for index: 10\n",
      "Finished QA for index: 18\n",
      "Calculating metrics for index: 18\n",
      "Finished metrics for index: 18\n",
      "Finished QA for index: 11\n",
      "Calculating metrics for index: 11\n",
      "Finished metrics for index: 11\n",
      "Finished QA for index: 7\n",
      "Calculating metrics for index: 7\n",
      "Finished metrics for index: 7\n",
      "Finished QA for index: 17\n",
      "Calculating metrics for index: 17\n",
      "Finished metrics for index: 17\n",
      "Finished QA for index: 1\n",
      "Calculating metrics for index: 1\n",
      "Finished metrics for index: 1\n",
      "Finished QA for index: 14\n",
      "Calculating metrics for index: 14\n",
      "Finished metrics for index: 14\n",
      "Finished QA for index: 5\n",
      "Calculating metrics for index: 5\n",
      "Finished metrics for index: 5\n",
      "Finished QA for index: 8\n",
      "Calculating metrics for index: 8\n",
      "Finished metrics for index: 8\n",
      "Finished QA for index: 13\n",
      "Calculating metrics for index: 13\n",
      "Finished metrics for index: 13\n",
      "Finished QA for index: 0\n",
      "Calculating metrics for index: 0\n",
      "Finished metrics for index: 0\n",
      "Finished QA for index: 15\n",
      "Calculating metrics for index: 15\n",
      "Finished metrics for index: 15\n",
      "Finished QA for index: 6\n",
      "Calculating metrics for index: 6\n",
      "Finished metrics for index: 6\n",
      "Finished QA for index: 4\n",
      "Calculating metrics for index: 4\n",
      "Finished metrics for index: 4\n",
      "Finished QA for index: 3\n",
      "Calculating metrics for index: 3\n",
      "Finished metrics for index: 3\n",
      "Finished QA for index: 16\n",
      "Calculating metrics for index: 16\n",
      "Finished metrics for index: 16\n",
      "\n",
      "--- Evaluation Complete ---\n",
      "\n",
      "Processed 19 samples successfully.\n",
      "Average Context Precision: 1.0000\n",
      "Average Context Recall: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import concurrent.futures\n",
    "from tqdm.auto import tqdm # Progress bar library\n",
    "# -------------------------------------\n",
    "# 5. Scale up!!!!\n",
    "# -------------------------------------\n",
    "\n",
    "# --- Worker Function ---\n",
    "# This function processes a single entry from the test data\n",
    "def process_entry(args):\n",
    "    \"\"\"\n",
    "    Processes a single test data entry: runs QA, prepares sample, calculates RAGAS metrics.\n",
    "    Args:\n",
    "        args (tuple): A tuple containing (index, entry_dict).\n",
    "    Returns:\n",
    "        dict: A dictionary containing results or error information.\n",
    "    \"\"\"\n",
    "    index, entry = args\n",
    "    try:\n",
    "        # 1. Run QA Chain (Synchronous I/O bound task)\n",
    "        print(f\"Starting QA for index: {index}\") # Optional: Track start\n",
    "        res = qa_chain({\"query\": entry[\"question\"]})\n",
    "        print(f\"Finished QA for index: {index}\") # Optional: Track end\n",
    "\n",
    "        # 2. Extract Contexts\n",
    "        retrieved_contexts = [doc.page_content for doc in res.get(\"source_documents\", [])]\n",
    "\n",
    "        # Ensure reference_contexts is a list of strings\n",
    "        reference_contexts_raw = entry.get(\"context\", []) # Get reference context(s)\n",
    "        if isinstance(reference_contexts_raw, str):\n",
    "            reference_contexts = [reference_contexts_raw]\n",
    "        elif isinstance(reference_contexts_raw, list):\n",
    "            reference_contexts = reference_contexts_raw\n",
    "        else:\n",
    "            print(f\"Warning: Unexpected type for reference context at index {index}. Treating as empty list.\")\n",
    "            reference_contexts = []\n",
    "\n",
    "        # 3. Prepare RAGAS Sample\n",
    "        # Adjust fields based on the metrics you are using.\n",
    "        # Here assuming only context metrics are needed.\n",
    "        sample = SingleTurnSample(\n",
    "            retrieved_contexts=retrieved_contexts,\n",
    "            reference_contexts=reference_contexts\n",
    "            # You might need question, answer, etc. for other RAGAS metrics\n",
    "            # question=entry[\"question\"],\n",
    "            # answer=res.get(\"result\", \"\")\n",
    "        )\n",
    "\n",
    "        # 4. Calculate Metrics (Run async metrics concurrently)\n",
    "        async def calculate_metrics_async(sample_to_score):\n",
    "            \"\"\"Helper async function to gather metric scores.\"\"\"\n",
    "            print(f\"Calculating metrics for index: {index}\") # Optional\n",
    "            # Add more metric tasks here if needed\n",
    "            precision_task = context_precision.single_turn_ascore(sample_to_score)\n",
    "            recall_task = context_recall.single_turn_ascore(sample_to_score)\n",
    "\n",
    "            # Run concurrently and wait for results\n",
    "            precision_score, recall_score = await asyncio.gather(\n",
    "                precision_task,\n",
    "                recall_task\n",
    "                # Add other awaited tasks here\n",
    "            )\n",
    "            print(f\"Finished metrics for index: {index}\") # Optional\n",
    "            return precision_score, recall_score\n",
    "\n",
    "        # Use asyncio.run to execute the async helper from this sync thread\n",
    "        precision, recall = asyncio.run(calculate_metrics_async(sample))\n",
    "\n",
    "        # 5. Prepare result dictionary\n",
    "        result_data = {\n",
    "            \"index\": index,\n",
    "            \"question\": entry[\"question\"],\n",
    "            \"retrieved_contexts\": retrieved_contexts,\n",
    "            \"reference_contexts\": reference_contexts,\n",
    "            \"context_precision\": precision,\n",
    "            \"context_recall\": recall,\n",
    "            \"raw_qa_response\": res # Optional: store full response\n",
    "        }\n",
    "        # Optional: print immediate results per sample\n",
    "        # print(f\"METRICS FOR INDEX {index}: CP={precision:.4f}, CR={recall:.4f}\")\n",
    "        return result_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing index {index} - Question: {entry.get('question', 'N/A')}: {e}\")\n",
    "        # Optionally log the full traceback\n",
    "        # import traceback\n",
    "        # traceback.print_exc()\n",
    "        return {\n",
    "            \"index\": index,\n",
    "            \"question\": entry.get('question', 'N/A'),\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "        \n",
    "# --- Main Execution ---\n",
    "all_results = []\n",
    "# Prepare arguments for mapping (index, entry) tuples\n",
    "tasks = list(enumerate(test_data)) # Creates [(0, test_data[0]), (1, test_data[1]), ...]\n",
    "\n",
    "# Use ThreadPoolExecutor for parallel execution\n",
    "num_workers = 20\n",
    "print(f\"Starting evaluation for {len(tasks)} samples with {num_workers} workers...\")\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "    # Use executor.map to process tasks in parallel.\n",
    "    # Results will be yielded in the order tasks were submitted (or close to it).\n",
    "    # Wrap with tqdm for a progress bar.\n",
    "    results_iterator = list(tqdm(executor.map(process_entry, tasks), total=len(tasks), desc=\"Evaluating Samples\"))\n",
    "\n",
    "all_results = results_iterator # list() consumes the iterator and gathers all results\n",
    "\n",
    "print(\"\\n--- Evaluation Complete ---\")\n",
    "\n",
    "# --- Post-Processing (Example) ---\n",
    "valid_results = [r for r in all_results if 'error' not in r]\n",
    "errors = [r for r in all_results if 'error' in r]\n",
    "\n",
    "if valid_results:\n",
    "    avg_precision = sum(r['context_precision'] for r in valid_results) / len(valid_results)\n",
    "    avg_recall = sum(r['context_recall'] for r in valid_results) / len(valid_results)\n",
    "    print(f\"\\nProcessed {len(valid_results)} samples successfully.\")\n",
    "    print(f\"Average Context Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average Context Recall: {avg_recall:.4f}\")\n",
    "else:\n",
    "    print(\"No samples processed successfully.\")\n",
    "\n",
    "if errors:\n",
    "    print(f\"\\nEncountered {len(errors)} errors during processing.\")\n",
    "    # Optionally print details of errors\n",
    "    # for error_result in errors:\n",
    "    #     print(f\"  Index {error_result['index']}: {error_result['error']}\")\n",
    "\n",
    "# Now `all_results` contains a list of dictionaries, each holding the\n",
    "# computed metrics and other info for one entry from test_data, or error details.\n",
    "# print(all_results[0]) # Example: Inspect the first result\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
